We propose a tightly-coupled LiDAR-visual SLAM based on geometric features, which develops a dual-system framework allowing two SLAM algorithms to run simultaneously. In the visual subsystem, we detect lines and associate depth and direction information from LiDAR geometric features to reconstruct 3D lines. Some of these reconstructed lines are employed to estimate the initial pose as new landmarks, others are used as additional parameters to constrain the line direction in the visual back-end. The LiDAR subsystem adjusts the direction of its linear features based on the complete line detected by the visual system, reducing the outlier likelihood during the registration process. These modifications further improve the accuracy of our system. In addition, we implement a detection module to detect the operation of the subsystems. If the visual subsystem fails, the LiDAR odometry is provided as system output to improve robustness.

The proposed method has demonstrated superior robustness compared to the standalone sensor frameworks, especially in scenarios involving high-speed sensor motion and significant lighting changes. Despite utilizing fewer sensors, our approach outperforms state-of-the-art methods in most cases. Our algorithm helps the mobile agent achieve localization and mapping in multiple scenarios, which enables the robot to accomplish tasks efficiently even with limited computational resources. We hope that we will obtain additional constraints between the odometry of the two subsystems to further enhance the system's performance.