\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\def\UrlFont{\rmfamily}
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{sisman2020overview}
B.~Sisman, J.~Yamagishi, S.~King, and H.~Li, ``An overview of voice conversion
  and its challenges: From statistical modeling to deep learning,''
  \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  vol.~29, pp. 132--157, 2020.

\bibitem{walczyna2023overview}
T.~Walczyna and Z.~Piotrowski, ``Overview of voice conversion methods based on
  deep learning,'' \emph{Applied Sciences}, vol.~13, no.~5, p. 3100, 2023.

\bibitem{qian2019autovc}
K.~Qian, Y.~Zhang, S.~Chang, X.~Yang, and M.~Hasegawa-Johnson, ``{AutoVC}:
  Zero-shot voice style transfer with only autoencoder loss,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2019, pp. 5210--5219.

\bibitem{yuan2021improving}
S.~Yuan, P.~Cheng, R.~Zhang, W.~Hao, Z.~Gan, and L.~Carin, ``Improving
  zero-shot voice style transfer via disentangled representation learning,'' in
  \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem{wang2021vqmivc}
D.~Wang, L.~Deng, Y.~T. Yeung, X.~Chen, X.~Liu, and H.~Meng, ``{VQMIVC}: Vector
  quantization and mutual information-based unsupervised speech representation
  disentanglement for one-shot voice conversion,'' in \emph{Proc. Interspeech},
  2021, pp. 1344--1348.

\bibitem{chen2021again}
Y.-H. Chen, D.-Y. Wu, T.-H. Wu, and H.-y. Lee, ``{Again-VC}: A one-shot voice
  conversion using activation guidance and adaptive instance normalization,''
  in \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2021, pp. 5954--5958.

\bibitem{lian2022robust}
J.~Lian, C.~Zhang, and D.~Yu, ``Robust disentangled variational speech
  representation learning for zero-shot voice conversion,'' in \emph{ICASSP
  2022-2022 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp.
  6572--6576.

\bibitem{casanova2022yourtts}
E.~Casanova, J.~Weber, C.~D. Shulby, A.~C. Junior, E.~G{\"o}lge, and M.~A.
  Ponti, ``{YourTTS}: Towards zero-shot multi-speaker tts and zero-shot voice
  conversion for everyone,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2022, pp. 2709--2720.

\bibitem{levkovitch2022zero}
A.~Levkovitch, E.~Nachmani, and L.~Wolf, ``Zero-shot voice conditioning for
  denoising diffusion tts models,'' in \emph{Proc. Interspeech}, 2022, pp.
  2983--2987.

\bibitem{li2023styletts}
Y.~A. Li, C.~Han, and N.~Mesgarani, ``{StyleTTS-VC}: One-shot voice conversion
  by knowledge transfer from style-based tts models,'' in \emph{2022 IEEE
  Spoken Language Technology Workshop (SLT)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2023, pp. 920--927.

\bibitem{hussain2023ace}
S.~Hussain, P.~Neekhara, J.~Huang, J.~Li, and B.~Ginsburg, ``{ACE-VC}: Adaptive
  and controllable voice conversion using explicitly disentangled
  self-supervised speech representations,'' \emph{arXiv preprint
  arXiv:2302.08137}, 2023.

\bibitem{choi2021neural}
H.-S. Choi, J.~Lee, W.~Kim, J.~Lee, H.~Heo, and K.~Lee, ``Neural analysis and
  synthesis: Reconstructing speech from self-supervised representations,''
  \emph{Advances in Neural Information Processing Systems}, vol.~34, pp.
  16\,251--16\,265, 2021.

\bibitem{qian2022contentvec}
K.~Qian, Y.~Zhang, H.~Gao, J.~Ni, C.-I. Lai, D.~Cox, M.~Hasegawa-Johnson, and
  S.~Chang, ``{ContentVec}: An improved self-supervised speech representation
  by disentangling speakers,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2022, pp.
  18\,003--18\,017.

\bibitem{dang2022training}
T.~Dang, D.~Tran, P.~Chin, and K.~Koishida, ``Training robust zero-shot voice
  conversion models with self-supervised features,'' in \emph{ICASSP 2022-2022
  IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 6557--6561.

\bibitem{zhang2020gazev}
Z.~Zhang, B.~He, and Z.~Zhang, ``{GAZEV}: {GAN}-based zero-shot voice
  conversion over non-parallel speech corpus,'' in \emph{Proc. Interspeech},
  2020, pp. 791--795.

\bibitem{nguyen2022nvc}
B.~Nguyen and F.~Cardinaux, ``{NVC-Net}: End-to-end adversarial voice
  conversion,'' in \emph{ICASSP 2022-2022 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2022, pp. 7012--7016.

\bibitem{takahashi2022robust}
N.~Takahashi, M.~K. Singh, and Y.~Mitsufuji, ``Robust one-shot singing voice
  conversion,'' \emph{arXiv preprint arXiv:2210.11096}, 2022.

\bibitem{yasur2023deepfake}
L.~Yasur, G.~Frankovits, F.~M. Grabovski, and Y.~Mirsky, ``{Deepfake CAPTCHA}:
  A method for preventing fake calls,'' \emph{arXiv preprint arXiv:2301.03064},
  2023.

\bibitem{li2023styletts2}
Y.~A. Li, C.~Han, V.~S. Raghavan, G.~Mischler, and N.~Mesgarani, ``Styletts 2:
  Towards human-level text-to-speech through style diffusion and adversarial
  training with large speech language models,'' \emph{arXiv preprint
  arXiv:2306.07691}, 2023.

\bibitem{li2021starganv2}
Y.~A. Li, A.~Zare, and N.~Mesgarani, ``{StarGANv2-VC}: A diverse, unsupervised,
  non-parallel framework for natural-sounding voice conversion,'' in
  \emph{Proc. Interspeech}, 2021, pp. 1349--1353.

\bibitem{lee2022bigvgan}
S.-g. Lee, W.~Ping, B.~Ginsburg, B.~Catanzaro, and S.~Yoon, ``{BigVGAN}: A
  universal neural vocoder with large-scale training,'' in \emph{International
  Conference on Learning Representations (ICLR)}, 2023.

\bibitem{chen2022wavlm}
S.~Chen, C.~Wang, Z.~Chen, Y.~Wu, S.~Liu, Z.~Chen, J.~Li, N.~Kanda,
  T.~Yoshioka, X.~Xiao, \emph{et~al.}, ``{WavLM}: Large-scale self-supervised
  pre-training for full stack speech processing,'' \emph{IEEE Journal of
  Selected Topics in Signal Processing}, vol.~16, no.~6, pp. 1505--1518, 2022.

\bibitem{huang2017arbitrary}
X.~Huang and S.~Belongie, ``Arbitrary style transfer in real-time with adaptive
  instance normalization,'' in \emph{Proceedings of the IEEE International
  Conference on Computer Vision (ICCV)}, 2017, pp. 1501--1510.

\bibitem{kum2019joint}
S.~Kum and J.~Nam, ``Joint detection and classification of singing voice melody
  using convolutional recurrent neural networks,'' \emph{Applied Sciences},
  vol.~9, no.~7, p. 1324, 2019.

\bibitem{mao2017least}
X.~Mao, Q.~Li, H.~Xie, R.~Y. Lau, Z.~Wang, and S.~Paul~Smolley, ``Least squares
  generative adversarial networks,'' in \emph{Proceedings of the IEEE
  International Conference on Computer Vision (ICCV)}, 2017, pp. 2794--2802.

\bibitem{pasad2021layer}
A.~Pasad, J.-C. Chou, and K.~Livescu, ``Layer-wise analysis of a
  self-supervised speech representation model,'' in \emph{2021 IEEE Automatic
  Speech Recognition and Understanding Workshop (ASRU)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2021, pp. 914--921.

\bibitem{yamagishi2019cstr}
J.~Yamagishi, C.~Veaux, K.~MacDonald, \emph{et~al.}, ``{CSTR VCTK} corpus:
  English multi-speaker corpus for cstr voice cloning toolkit (version 0.92),''
  2019.

\bibitem{loshchilov2018fixing}
\BIBentryALTinterwordspacing
I.~Loshchilov and F.~Hutter, ``Fixing weight decay regularization in {Adam},''
  2018. [Online]. Available: \url{https://openreview.net/forum?id=rk6qdGgCZ}
\BIBentrySTDinterwordspacing

\bibitem{zhao2021improved}
Z.~Zhao, S.~Singh, H.~Lee, Z.~Zhang, A.~Odena, and H.~Zhang, ``Improved
  consistency regularization for {GANs},'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~35, no.~12, 2021, pp.
  11\,033--11\,041.

\bibitem{li2022styletts}
Y.~A. Li, C.~Han, and N.~Mesgarani, ``{StyleTTS}: A style-based generative
  model for natural and diverse text-to-speech synthesis,'' \emph{arXiv
  preprint arXiv:2205.15439}, 2022.

\bibitem{watanabe2018espnet}
\BIBentryALTinterwordspacing
S.~Watanabe, T.~Hori, S.~Karita, T.~Hayashi, J.~Nishitoba, Y.~Unno, N.~{Enrique
  Yalta Soplin}, J.~Heymann, M.~Wiesner, N.~Chen, A.~Renduchintala, and
  T.~Ochiai, ``{ESPnet}: End-to-end speech processing toolkit,'' in
  \emph{Proceedings of Interspeech}, 2018, pp. 2207--2211. [Online]. Available:
  \url{http://dx.doi.org/10.21437/Interspeech.2018-1456}
\BIBentrySTDinterwordspacing

\end{thebibliography}
