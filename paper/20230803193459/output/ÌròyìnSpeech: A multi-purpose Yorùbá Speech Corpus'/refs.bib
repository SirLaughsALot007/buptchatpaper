@InProceedings{C2,
  author = 	 "Jones, C.D. and Smith, A.B. and Roberts, E.F.",
  title =        "Article Title",
  booktitle =        "Proceedings Title",
  organization = "IEEE",
  year = 	 "2003",
  volume = 	 "II",
  pages = 	 "803-806"
}

@inproceedings{oktem2020gamayun,
  title={Gamayun-language technology for humanitarian response},
  author={{\"O}ktem, Alp and Jaam, Muhannad Albayk and DeLuca, Eric and Tang, Grace},
  booktitle={2020 IEEE Global Humanitarian Technology Conference (GHTC)},
  pages={1--4},
  year={2020},
  organization={IEEE}
}

@inproceedings{black2019cmu,
  title={Cmu wilderness multilingual speech dataset},
  author={Black, Alan W},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5971--5975},
  year={2019},
  organization={IEEE}
}

@inproceedings{adelani-etal-2021-effect,
    title = "The Effect of Domain and Diacritics in {Y}oruba{--}{E}nglish Neural Machine Translation",
    author = "Adelani, David  and
      Ruiter, Dana  and
      Alabi, Jesujoba  and
      Adebonojo, Damilola  and
      Ayeni, Adesina  and
      Adeyemi, Mofe  and
      Awokoya, Ayodele Esther  and
      Espa{\~n}a-Bonet, Cristina",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Research Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-research.6",
    pages = "61--75",
    abstract = "Massively multilingual machine translation (MT) has shown impressive capabilities and including zero and few-shot translation between low-resource language pairs. However and these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due to lack of standardized evaluation datasets. In this paper and we present MENYO-20k and the first multi-domain parallel corpus with a especially curated orthography for Yoruba{--}English with standardized train-test splits for benchmarking. We provide several neural MT benchmarks and compare them to the performance of popular pre-trained (massively multilingual) MT models both for the heterogeneous test set and its subdomains. Since these pre-trained models use huge amounts of data with uncertain quality and we also analyze the effect of diacritics and a major characteristic of Yoruba and in the training data. We investigate how and when this training condition affects the final quality of a translation and its understandability.Our models outperform massively multilingual models such as Google ($+8.7$ BLEU) and Facebook M2M ($+9.1$) when translating to Yoruba and setting a high quality benchmark for future research.",
}
@inproceedings{commonvoice,
    title = "Common Voice: A Massively-Multilingual Speech Corpus",
    author = "Ardila, Rosana  and
      Branson, Megan  and
      Davis, Kelly  and
      Kohler, Michael  and
      Meyer, Josh  and
      Henretty, Michael  and
      Morais, Reuben  and
      Saunders, Lindsay  and
      Tyers, Francis  and
      Weber, Gregor",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.520",
    pages = "4218--4222",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Related Work (compiled by iroro)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{Odejobi2004ACM,
  title={A Computational Model of Intonation for Yor{\`u}b{\'a} Text-to-Speech Synthesis: Design and Analysis},
  author={{Od{\'e}t{\'u}nj{\'\i}} {\`A}j{\`a}d{\'\i} Od{\'e}job{\'\i} and Anthony Joseph Beaumont and Shun Ha Sylvia Wong},
  booktitle={TSD},
  year={2004}
}

@article{ajadi2007quantitative,
  title={A quantitative model of yor{\`u}b{\'a} speech intonation using stem-ml},
  author={{\`A}j{\`a}d{\'\i}, Od{\'e}job{\'\i} Od{\'e}t{\'u}nj{\'\i}},
  journal={INFOCOMP Journal of Computer Science},
  year={2007}
}

@inproceedings{Niekerk2012ToneRI,
  title={Tone realisation in a yor{\`u}b{\'a} speech recognition corpus},
  author={Daniel R. van Niekerk and Etienne Barnard},
  booktitle={SLTU},
  year={2012}
}

@article{Akinwonmi2013APT,
  title={A prosodic Text-to-Speech system for Yor{\`u}b{\'a} language},
  author={Akintoba Emmanuel Akinwonmi and B. K. Alese},
  journal={ICITST},
  year={2013},
}

@inproceedings{ynd2014DevelopmentOG,
  title={Development of grapheme-to-phoneme conversion system for yor{\`u}b{\'a} text-to-speech synthesis},
  author={Ab{\'i}mb{\'o}l{\'a} Rhoda {\`I}y{\`a}nd{\'a} and Odetunji Ajadi Odejobi and Festus Ayodeji Soyoye and Ol{\'u}b{\'e}nga O. Akinad{\'e}},
  year={2014}
}

@article{akinade2014computational,
  title={Computational modelling of Yor{\`u}b{\'a} numerals in a number-to-text conversion system},
  author={Akinad{\'e}, Ol{\'u}gb{\'e}nga O and \d{O}d\d{\'e}j\d{o}b{\'\i}, \d{O}d\d{\'e}t{\'u}nj{\'\i} A},
  journal={Journal of Language Modelling},
  year={2014}
}

@inproceedings{Afolabi2014DevelopmentOT,
  title={Development of Text to Speech System for Yoruba Language},
  author={Akingbemisilu Abiola Afolabi and Elijah Olusayo Omidiora and O. T. Arulogun},
  year={2014}
}

@article{van2015lagos,
  title={Lagos-NWU Yoruba speech corpus},
  author={van Niekerk, Daniel and Barnard, Etienne and Giwa, Oluwapelumi and Sosimi, Azeez},
  year={2015},
  publisher={North-West University}
}

@article{aoga2016integration,
  title={Integration of Yoruba language into MaryTTS},
  author={Aoga, John OR and Dagba, Theophile K and Fanou, Codjo C},
  journal={International Journal of Speech Technology},
  year={2016},
}

@inproceedings{Dagba2016DesignOA,
  title={Design of a Yoruba Language Speech Corpus for the Purposes of Text-to-Speech (TTS) Synthesis},
  author={Th{\'e}ophile K. Dagba and John O. R. Aoga and Codjo C. Fanou},
  booktitle={ACIIDS},
  year={2016}
}

@article{iyanda2017development,
  title={Development of a Yor{\'u}b{\`a} Textto-Speech System Using Festival},
  author={Iyanda, Abimbola Rhoda and Ninan, Olufemi Deborah},
  journal={Innovative Systems Design and Engineering (ISDE)},
  volume={8},
  number={5},
  year={2017}
}

@article{gutkin2020developing,
  title={Developing an open-source corpus of yoruba speech},
  author={Gutkin, Alexander and Demirsahin, Isin and Kjartansson, Oddur and Rivera, Clara E and T{\'u}b{\`o}s{\'u}n, K{\'o}l{\'a}},
  year={2020}
}

@article{meyer2022bibletts,
  title={BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus},
  author={Meyer, Josh and Adelani, David Ifeoluwa and Casanova, Edresson and {\"O}ktem, Alp and Weber, Daniel Whitenack Julian and Kabongo, Salomon and Salesky, Elizabeth and Orife, Iroro and Leong, Colin and Ogayo, Perez and others},
  journal={arXiv preprint arXiv:2207.03546},
  year={2022}
}







@misc{Ethnologue2019,
    author = "David M. Eberhard and Gary F. Simons and Charles D. Fennig (eds.)",
    title = "Ethnologue: Languages of the World. Twenty-second edition.",
    year = "2019",
    organization = "SIL International",
    url = "http://www.ethnologue.com"
}


@article{Asahiah2017,
	author = {Asahiah, F. O. and Odejobi, O. A. and Adagunodo, E. R.},
	year = "2017",
	title={Restoring Tone-Marks in Standard Yoruba Electronic Text: Improved Model},
	journal = {Computer Science},
	volume = "18",
	number = "3",
	pages = "301--315"
}

@inproceedings{adegbolaOdi12,
  title={Quantifying the effect of corpus size on the quality of automatic diacritization of Yoruba texts},
  author={Adegbola, Tunde and Odilinye, Lydia U. },
  booktitle={Spoken Language Technologies for Under-Resourced Languages},
  pages={},
  year={2012},
}

@inproceedings{adelani-etal-2022-masakhaner,
    title = "{M}asakha{NER} 2.0: {A}frica-centric Transfer Learning for Named Entity Recognition",
    author = "Adelani, David  and
      Neubig, Graham  and
      Ruder, Sebastian  and
      Rijhwani, Shruti  and
      Beukman, Michael  and
      Palen-Michel, Chester  and
      Lignos, Constantine  and
      Alabi, Jesujoba  and
      Muhammad, Shamsuddeen  and
      Nabende, Peter  and
      Dione, Cheikh M. Bamba  and
      Bukula, Andiswa  and
      Mabuya, Rooweither  and
      Dossou, Bonaventure F. P.  and
      Sibanda, Blessing  and
      Buzaaba, Happy  and
      Mukiibi, Jonathan  and
      Kalipe, Godson  and
      Mbaye, Derguene  and
      Taylor, Amelia  and
      Kabore, Fatoumata  and
      Emezue, Chris Chinenye  and
      Aremu, Anuoluwapo  and
      Ogayo, Perez  and
      Gitau, Catherine  and
      Munkoh-Buabeng, Edwin  and
      Memdjokam Koagne, Victoire  and
      Tapo, Allahsera Auguste  and
      Macucwa, Tebogo  and
      Marivate, Vukosi  and
      Elvis, Mboning Tchiaze  and
      Gwadabe, Tajuddeen  and
      Adewumi, Tosin  and
      Ahia, Orevaoghene  and
      Nakatumba-Nabende, Joyce  and
      Mokono, Neo Lerato  and
      Ezeani, Ignatius  and
      Chukwuneke, Chiamaka  and
      Oluwaseun Adeyemi, Mofetoluwa  and
      Hacheme, Gilles Quentin  and
      Abdulmumin, Idris  and
      Ogundepo, Odunayo  and
      Yousuf, Oreen  and
      Moteu, Tatiana  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.298",
    pages = "4488--4508",
    abstract = "African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and recently proposed methods like cross-lingual transfer will be effective. In this paper, we aim to move towards solutions for these challenges, focusing on the task of named entity recognition (NER). We present the creation of the largest to-date human-annotated NER dataset for 20 African languages. We study the behaviour of state-of-the-art cross-lingual transfer methods in an Africa-centric setting, empirically demonstrating that the choice of source transfer language significantly affects performance. While much previous work defaults to using English as the source language, our results show that choosing the best transfer language improves zero-shot F1 scores by an average of 14{\%} over 20 languages as compared to using English.",
}


@inproceedings{gutkin_49562,
title	= {Developing an Open-Source Corpus of Yoruba Speech},
author	= {Alexander Gutkin and Isin Demirsahin and Oddur Kjartansson and Clara E. Rivera and Kólá Túbòsún},
year	= {2020},
URL	= {http://dx.doi.org/10.21437/Interspeech.2020-1096},
booktitle	= {Proc. of Interspeech 2020},
pages	= {404--408},
address	= {October 25--29, Shanghai, China, 2020.}
}



@inproceedings{meyer22c_interspeech,
  author={Josh Meyer and David Adelani and Edresson Casanova and Alp Öktem and Daniel Whitenack and Julian Weber and Salomon {Kabongo Kabenamualu} and Elizabeth Salesky and Iroro Orife and Colin Leong and Perez Ogayo and Chris {Chinenye Emezue} and Jonathan Mukiibi and Salomey Osei and Apelete Agbolo and Victor Akinode and Bernard Opoku and Olanrewaju Samuel and Jesujoba Alabi and Shamsuddeen Hassan Muhammad},
  title={{BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={2383--2387},
  doi={10.21437/Interspeech.2022-10850}
}