
%% bare_jrnl_comsoc.tex
%% V1.4\elseb
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Communications Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.   ***   ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal,comsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal,comsoc]{../sty/IEEEtran}


\usepackage[T1]{fontenc}% optional T1 font encoding
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{url}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{bbding}

\urlstyle{same}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\cmmnt}[1]{}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
% Do NOT use the amsbsy package under comsoc mode as that feature is
% already built into the Times Math font (newtxmath, mathtime, etc.).
% 
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% Select a Times math font under comsoc mode or else one will automatically
% be selected for you at the document start. This is required as Communications
% Society journals use a Times, not Computer Modern, math font.
\usepackage[cmintegrals]{newtxmath}
% The freely available newtxmath package was written by Michael Sharpe and
% provides a feature rich Times math font. The cmintegrals option, which is
% the default under IEEEtran, is needed to get the correct style integral
% symbols used in Communications Society journals. Version 1.451, July 28,
% 2015 or later is recommended. Also, do *not* load the newtxtext.sty package
% as doing so would alter the main text font.
% http://www.ctan.org/pkg/newtx
%
% Alternatively, you can use the MathTime commercial fonts if you have them
% installed on your system:
%\usepackage{mtpro2}
%\usepackage{mt11p}
%\usepackage{mathtime}


%\usepackage{bm}
% The bm.sty package was written by David Carlisle and Frank Mittelbach.
% This package provides a \bm{} to produce bold math symbols.
% http://www.ctan.org/pkg/bm





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).     ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% Notes/comments
\newcommand{\MH}[1]{\textcolor{red}{#1}}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
%\title{Controllable cross-speaker emotion transfer for end-to-end speech synthesis}
%Kaleidoscope Speech: cross-speaker and cross-lingual emotion transfer based on diffusion model
\title{METTS: Multilingual Emotional Text-to-Speech by Cross-speaker and Cross-lingual Emotion Transfer}
%
\author{Xinfa Zhu,
        Yi Lei,
        Tao Li,
        Yongmao Zhang,
        Hongbin Zhou,
        Heng Lu,
        Lei Xie,~\IEEEmembership{Senior Member,~IEEE}
% \vspace{-0.4cm}
  % <-this % stops a space
 \thanks{Corresponding author: Lei Xie}

\thanks{Xinfa Zhu, Yi Lei, Tao Li, Yongmao Zhang, and Lei Xie are with Audio, Speech and Language Processing Group (ASLP@NPU), the School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China. Email: xfzhu@mail.nwpu.edu.cn (Xinfa Zhu), leiyi@npu-aslp.org (Yi Lei), taoli@npu-aslp.org (Tao Li), zym@mail.nwpu.edu.cn (Yongmao Zhang), lxie@nwpu.edu.cn (Lei Xie)}

% \thanks{Chenxu Hu is with the Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing 100084, China, Email: chenxuhu65@gmail.com}
% \thanks{Jingbei Li is with the Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China, Email: lijb19@mails.tsinghua.edu.cn}
% hu-cx21@mails.tsinghua.edu.cn
\thanks{Hongbin Zhou and Heng Lu are with Ximalaya Inc., Shanghai, China. Email: hongbin.zhou@ximalaya.com (Hongbin Zhou), bear.lu@ximalaya.com (Heng Lu)}
    }

%\thanks{Xinsheng Wang is with the School of Software Engineering, Xi’an Jiaotong University, Xi’an 710049, China, and also with the School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, China. Email: wangxinsheng@stu.xjtu.edu.cn}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%     ^------------^------------^----Do not want these spaces!

%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Wang \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Communications Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.       ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.

% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% make the title area
\maketitle


\begin{abstract}

Previous multilingual text-to-speech (TTS) approaches have considered leveraging monolingual speaker data to enable cross-lingual speech synthesis. However, such data-efficient approaches have ignored synthesizing emotional aspects of speech due to the challenges of cross-speaker cross-lingual emotion transfer -- the heavy entanglement of \textit{speaker timbre}, \textit{emotion} and \textit{language} factors in the speech signal will make a system to produce cross-lingual synthetic speech with an undesired foreign accent and weak emotion expressiveness. This paper proposes Multilingual Emotional TTS (METTS) model to mitigate these problems, realizing both cross-speaker and cross-lingual emotion transfer. Specifically, METTS takes DelightfulTTS as the backbone model and proposes the following designs. First, to alleviate the foreign accent problem, METTS introduces~\textit{multi-scale emotion modeling} to disentangle speech prosody into coarse-grained and fine-grained scales, producing language-agnostic and language-specific emotion representations, respectively. Second, as a pre-processing step, formant shift based \textit{information perturbation} is applied to the reference signal for better disentanglement of speaker timbre in the speech. Third, a vector quantization based \textit{emotion matcher} is designed for reference selection, leading to decent naturalness and emotion diversity in cross-lingual synthetic speech. Experiments demonstrate the good design of METTS.
\end{abstract}

\begin{IEEEkeywords}
Speech synthesis, cross-lingual, emotion transfer, disentanglement, diffusion model
\end{IEEEkeywords}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

% \vspace{-0.2cm}
\section{Introduction}
\label{sc:Introduction}

\IEEEPARstart{R}{ecent} years have witnessed significant progress in the quality and naturalness of synthetic speech thanks to the advances of neural text-to-speech (TTS) systems~\cite{DBLP:conf/interspeech/WangSSWWJYXCBLA17,DBLP:conf/nips/RenRTQZZL19,DBLP:conf/aaai/Li0LZL19,DBLP:conf/iclr/Chen0LLQZL21,DBLP:conf/interspeech/JeongKCCK21,DBLP:conf/icassp/WeissSBMK21,DBLP:conf/icml/KimKS21,DBLP:journals/taslp/LeiYWX22}. As near human parity performance has been reported in some closed TTS domains~\cite{DBLP:conf/acl/0006TQZL22}, \textit{diversity} and \textit{controllability} have become the new chasing target, including multi-speaker~\cite{DBLP:conf/icassp/WuSLW22}, multi-lingual~\cite{DBLP:conf/interspeech/ZhangWZWCSJRR19}, multi-emotion~\cite{DBLP:journals/taslp/LeiYWX22} as well as multi-style~\cite{DBLP:conf/icml/MinLYH21} scenarios. At the same time, \textit{data efficiency} is also highly desired as modern corpus-based TTS heavily relies on high-quality annotated data. This induces a variety of approaches better leveraging limited, low-resource as well as low-quality data~\cite{DBLP:conf/icassp/LiuYSY22,DBLP:conf/icassp/Yan0LQZSL21,DBLP:conf/interspeech/SaekiTY22}. 

This paper addresses an extremely diverse and controllable speech generation scenario -- multilingual emotional text-to-speech (METTS), particularly considering data efficiency by cross-speaker and cross-lingual emotion transfer. Specifically, METTS can produce bilingual emotional speech for each speaker after system building, while in the training data, each speaker speaks only one language (monolingual speaker), and some speakers have only neutral speech. Importantly, with only the neutral native speech (L1) data for a target speaker during training, helped with another emotional speaker in the target language (L2), our METTS system is able to produce the target speaker's emotional speech in the target language (L2) with reasonable proficiency and naturalness. METTS has significant applications such as foreign movie dubbing and computer-assisted language learning (CALL). However, building METTS is not a trivial task with three challenges.

\begin{itemize}
  \item

\textit{Foreign accent problem}. Different languages have quite different prosody patterns in pronunciation. In a typical cross-lingual TTS system, the accent of the source language may be inevitably delivered to the speech in the target language~\cite{DBLP:conf/interspeech/ZhangWZWCSJRR19}, leading to non-native synthetic speech with a strong foreign accent. This problem is prominent for cross-speaker and cross-lingual emotion transfer because emotional expressions are heavily reflected in prosody patterns through changes in pitch, loudness, speech rate and pauses.

\item

 \textit{Speech entanglement problem}. 
Only partial speakers in the training set have emotional speech data, while we need to enable each speaker in the training set generates emotional speech. The speaker timbre and emotion entanglement in speech may lead to \textit{speaker timbre leakage} when performing cross-speaker emotion transfer~\cite{DBLP:journals/spl/LeiYZXS22}. In other words, the source speaker's timbre may be inevitably transferred to the speech of the target speaker, making the synthetic speech of the target speaker sounds like the source speaker. Particularly, this problem becomes more severe for cross-speaker cross-lingual emotion transfer as the entanglement of three factors in speech -- speaker timbre, language and emotion complicates the disentanglement process.
%In the cross-speaker and cross-lingual emotion transfer task, we only have monolingual and neutral data for target speakers. While performing emotion transfer, the source speaker timbre may be inevitably transferred to the speech of target speaker
 %Partial speakers have only one emotion or language, meaning that the language and emotion are entangled with the speaker timbre in speech. Whereas, it's quite challenging to simultaneously decouple both emotion and language with speaker timbre, which is rare to be focused by previous works. %Previous work has only decoupled the speaker from language or emotion, but we need to decouple the speaker from emotion and language at the same time.
\item
 \textit{Emotional diversity problem}. In order to synthesize diverse emotions, the TTS model usually needs additional emotional information as prior, such as an emotion ID or a reference speech sample. Compared to emotion ID, providing emotional representation through a reference encoder is apparently more diverse~\cite{DBLP:journals/corr/abs-2206-14866} as different references lead to different fine-grained emotion deliveries. However, such diversity raises a problem -- how to select an appropriate reference signal to exactly match the textual content~\cite{DBLP:conf/interspeech/MengL0LSXSZM22}. In this paper, cross-lingual reference selection, i.e., selecting a reference in L1 to match the text in L2, is a brand new problem for cross-speaker emotion transfer in the multilingual TTS system.
 
 %Different reference audio leads to different emotion perceptions in the synthetic speech, because of the various fine-grained detail even though from the same emotion category. It's critical to choose an appropriate reference signal to exactly match the textual content, which is difficult since there is no definite standard for the reference matching, especially challenging in cross-lingual emotion transfer. % However, there are differences in the synthetic audio perception of different references for the exact text and the same reference for different texts in a reference encoder-based model~\cite{DBLP:conf/interspeech/MengL0LSXSZM22}. Choosing an appropriate reference speech according to context is a challenging task, especially since it is difficult to understand the context in multiple languages.
\end{itemize}

To address the above challenges, we propose METTS, a novel approach to synthesizing bilingual emotional speech for each monolingual speaker, even though some speakers do not have emotional speech data during model training. METTS is based on DeligtfulTTS~\cite{DBLP:journals/corr/abs-2110-12612}, a state-of-the-art non-autoregressive text-to-speech approach with improved Conformer~\cite{DBLP:conf/interspeech/GulatiQCPZYHWZW20} blocks to model the variation of speech prosody. Based on the skeleton of DelightfulTTS, our METTS leverages the following designs to achieve: 1) emotion transfer from a reference mel-spectrogram to synthesize speech (METTS-REF) and 2) automatically matching the most suitable reference embedding according to the input text and emotion ID to synthesize speech (METTS-ID).

First, we introduce \textit{multi-scale emotion modeling} to address the foreign accent problem. 
% can be generally factorized into the similar and 
We believe emotion expressions in multilingual speech can be generally factorized into the shared similar prosody pattern~\cite{DBLP:conf/chi/DaiFM09, DBLP:conf/interspeech/SchullerMLR05} (e.g., high pitch for angry and low pitch for sad) conveyed in both languages and distinct fine details of prosody~\cite{DBLP:journals/ijst/KottiP12, DBLP:conf/mldm/FersiniMAA09} due to the different manners of pronunciation. %speech in different languages should share a similar general prosody pattern for the same emotion category~\cite{DBLP:conf/chi/DaiFM09, DBLP:conf/interspeech/SchullerMLR05} (e.g., high pitch for angry and low pitch for sad) while performing differently in fine details of prosody~\cite{DBLP:journals/ijst/KottiP12, DBLP:conf/mldm/FersiniMAA09} due to the different manners of pronunciation. 
This inspires us to model emotion with \textit{coarse} and \textit{fine} scales to respectively represent \textit{language-agnostic} and \textit{language-specific} emotion aspects in speech. To be specific, the coarse-grained emotion is modeled by a Global Style Token (GST)~\cite{DBLP:conf/icml/WangSZRBSXJRS18} layer with semi-supervised constraints to make the coarse-grained emotional representation language-agnostic; a Conditional Variational Autoencoder (CVAE) layer establishes a language-specific emotion representation by investigating the fine-grained prosody variation of speech, with the condition of language-dependent text input and the above coarse-grained emotional representation. In this way, METTS manages to successfully transfer emotion across languages via the coarse-grained language-agnostic emotional representation and produces native pronunciation without a foreign accent in emotion delivery through the fine-grained language-specific emotion representation. 

Second, we employ \textit{information perturbation}~\cite{DBLP:journals/spl/LeiYZXS22} to further address the issue of speech entanglement. Specifically, the reference speech is perturbed by randomly shifting its formant frequency, which allows the multi-scale emotion modeling module to generate a speaker-independent emotional representation. This speaker-independent signal can decouple the speaker timbre from reference speech in nature.

Third, to address the emotional diversity problem and select
an appropriate reference signal, we propose a vector quantization (VQ) based \textit{emotion matching} module. Instead of directly modeling the complicated regression between bilingual textual representation and emotional representation, we first use VQ to quantify coarse-grained language-agnostic emotion representation to form a reference pool and subsequently adopt an emotion matcher to match the bilingual textual representation with the reference pool. This allows METTS to realize \textit{reference-free} inference and produce more diverse emotional speech with reasonable expressiveness for both L1 and L2 text inputs.

The proposed METTS system is extensively evaluated on a demanding Mandarin-English bilingual TTS task. This task poses significant challenges due to substantial differences in pronunciation between Chinese and English, such as variations in syllable structure, tones, vowels, and consonant inventory, the absence of retroflex sounds in Chinese, and the presence of long vowels in English~\cite{han2013pronunciation,li2016contrastive}. Experimental results show that although the performance of intra-lingual emotional speech synthesis is better than that of more challenging cross-lingual emotional speech synthesis, METTS can produce bilingual emotional speech for each target speaker and effectively improve speech naturalness, speaker
similarity, and emotion similarity compared to other competitive methods. Furthermore, the component analysis validates the good design of our proposed model. Audio samples can be found on our demo page \footnote{https://anonymous-rep0.github.io/METTS/}.

% The proposed METTS system is extensively evaluated on a demanding Mandarin-English bilingual TTS task. This task poses significant challenges due to substantial differences in pronunciation between Chinese and English, such as variations in syllable structure, tones, vowels, and consonant inventory, the absence of retroflex sounds in Chinese, and the presence of long vowels in English [***]. In our experiments, we thoroughly assess the performance of METTS in intra-lingual emotional speech synthesis, which proves to be superior to the more demanding cross-lingual emotional speech synthesis.
% The experimental results demonstrate that METTS is capable of generating bilingual emotional speech for each target speaker and effectively enhances speech naturalness, speaker similarity, and emotion similarity compared to other competitive approaches. Furthermore, the component analysis validates the sound design of our proposed model. For audio samples of our system's output, please visit our demo page\footnote{https://anonymous-rep0.github.io/METTS/}.

% The rest of this paper is organized as follows. 
% Section~\ref{sc:related work} reviews the related work. 
% Section~\ref{sc:method} introduces the proposed approach in detail.  
% Section \ref{sc:experiments} and Section \ref{sc:results} describe the experimental setups and results, respectively.
% The component analysis is introduced in Section~\ref{component}. 
% Finally, the paper concludes in Section \ref{sc:conclusion}. 
The remainder of this paper is organized as follows. Section~\ref{sc:related work} provides a comprehensive review of related work in the field. Section~\ref{sc:method} presents a detailed description of the proposed approach. The experimental setups and results are described in Section~\ref{sc:experiments} and Section~\ref{sc:results}, respectively, where we analyze the performance of METTS and present the evaluation outcomes. In Section~\ref{component}, we delve into the component analysis, examining the contributions of each individual module in our system. Finally, Section~\ref{sc:conclusion} concludes the paper, summarizing the findings and highlighting future research directions.

%\vspace{-0.1cm}
\section{Related work}
\label{sc:related work}
Multi-lingual speech synthesis and emotional speech synthesis are two popular topics in the literature where transfer learning approaches -- transferring speaker voices across languages or transferring style/emotion across speakers -- are mainstream approaches better leveraging limited data. However, to the best of our knowledge, the current studies have not yet addressed both cross-speaker and cross-lingual emotion transfer. This is a more challenging task, as discussed in Section 1. Moreover, to improve the diversity of synthetic speech, cross-lingual reference selection is another new problem in reference-based multi-lingual emotional TTS. Therefore, here we review the prior arts in multilingual speech synthesis, emotional speech synthesis, and reference speech selection, respectively.

\subsection{Multilingual speech synthesis}

% To build a multilingual text-to-speech (TTS) system, a unified textual input is essential. This can be accomplished by merging the phoneme sets of different languages or adopting the International Phonetic Alphabet (IPA) to represent speech sounds. With a unified input format, some studies have explored using a single acoustic model incorporating shared parameters across different languages as an alternative to training separate models for each language.~\cite{DBLP:conf/ssw/SitaramRRB16,DBLP:conf/interspeech/XueSXXW19,DBLP:conf/interspeech/LiZ16,DBLP:conf/icassp/NachmaniW19}.

% However, as different languages have different prosody patterns during articulation, a multilingual TTS system faces a strong foreign accent problem in cross-lingual synthetic audio. To encourage the TTS model to learn disentangled
% representations of the text (from a specific language) and speaker identity, adversarial speaker training strategy is often used~\cite{DBLP:conf/interspeech/ZhangWZWCSJRR19,DBLP:conf/interspeech/NekvindaD20,DBLP:conf/interspeech/PengL22}, where a gradient reversal layer is typically inserted prior to a
% speaker classifier. Besides adopting adversarial speaker training, a conditional variational encoder (CVAE) was also utilized in~\cite{DBLP:conf/interspeech/ShangHZZ021,DBLP:conf/interspeech/RattcliffeWMKMC22} to alleviate the foreign accent problem in cross-lingual synthesis. 

Developing a robust multilingual text-to-speech (TTS) system requires a unified representation of textual input. This can be achieved by merging phoneme sets from different languages or utilizing the International Phonetic Alphabet (IPA) to represent speech sounds. Based on the unified representation of textual input, several studies have explored a more efficient method that utilizes a single acoustic model with shared parameters across languages as an alternative to training separate models for each language~\cite{DBLP:conf/ssw/SitaramRRB16,DBLP:conf/interspeech/XueSXXW19,DBLP:conf/icassp/NachmaniW19}. Additionally, incorporating explicit language identification (ID) enables better control over language-specific prosody and improves the naturalness of synthetic speech~\cite{DBLP:conf/interspeech/LiZ16, DBLP:conf/interspeech/PengL22}.

However, due to distinct prosody patterns in different languages, %language-ID-based 
cross-lingual synthetic audio often suffers from an undesired foreign accent problem in multilingual TTS systems. To address this challenge, some researchers investigate obtaining language-specific and speaker-independent prosodic representations through implicit disentanglement~\cite{DBLP:conf/interspeech/XinSTKS20,DBLP:conf/icassp/XinKTS21}. Typically, domain adversarial training strategies have been employed to encourage the model to learn disentangled representations of text and speaker identity~\cite{DBLP:conf/interspeech/ZhangWZWCSJRR19,DBLP:conf/interspeech/NekvindaD20,DBLP:conf/interspeech/PengL22}, where a gradient reversal layer is inserted before a speaker classifier. Furthermore, some studies~\cite{DBLP:conf/interspeech/ShangHZZ021,DBLP:conf/interspeech/RattcliffeWMKMC22} propose to use a style VAE encoder to alleviate the foreign accent problem by leveraging existing authentic styles during inference. Moreover, the triplet training scheme is utilized to overcome the accent problem by combining unseen speakers and language through fine-tuning~\cite{DBLP:conf/icassp/YeZSHRLL22}. CrossSpeech~\cite{DBLP:journals/corr/abs-2302-14370} obtains disentangled speaker and language representations through the speaker-independent generator and speaker-dependent generator.


It is important to note that while these approaches have shown promise in improving the performance of multilingual speech synthesis, challenges still remain in achieving emotionally expressive and foreign accent-free speech across different languages. %Future advancements in disentangling language-specific and speaker-related information hold the potential further to improve the quality and naturalness of multilingual TTS systems.




\subsection{Emotional speech synthesis}

% Emotional speech synthesis has made significant progress in recent years~\cite{DBLP:journals/corr/abs-2206-14866,DBLP:journals/taslp/LeiYWX22,DBLP:journals/taslp/LiWXWX22,DBLP:journals/corr/abs-2110-04153}. 
% When the categorized emotion data is available for the target speaker, a straightforward approach is to  model the emotional expressions based on discrete emotion ID~\cite{DBLP:conf/apsipa/AnLD17,DBLP:journals/corr/abs-1711-05447}. However, the delivered emotion is always over-averaged due to the discrete emotion ID control. By contrast, some approaches use transfer learning, that is, using a reference encoder to extract emotional representation from a reference signal as guidance for emotion synthesis. Such emotion transfer approaches are more flexible, and diverse emotional speech can be produced~\cite{DBLP:conf/icml/Skerry-RyanBXWS18,DBLP:conf/interspeech/AkuzawaIM18,DBLP:conf/iscslp/LiYXX21}. 

% Although emotion transfer is an effective way to generate emotional speech for a speaker who has only neutral data, emotion transfer also faces the problem of coupling between the speaker timbre and emotion attributes, resulting in either low speaker similarity or poor emotion similarity due to the trade-off between the two attributes. Disentangling these attributes and modeling them separately is a necessity.
% The schemes can be divided into those that perform decoupling outside~\cite{DBLP:conf/nips/ChoiLKLHL21,DBLP:conf/icassp/ChanQZH22} and inside~\cite{DBLP:conf/interspeech/WangDYCLM21,DBLP:conf/interspeech/WangLZ0KM21} the model. Similarly, adversarial training and orthogonal loss~\cite{DBLP:journals/taslp/LiWXWX22} are usually adopted.

% As the expression of human emotion is various and complex, some articles propose to model emotional speech in multiple scales to capture rich emotion variations~\cite{DBLP:journals/taslp/LeiYWX22,DBLP:journals/corr/abs-2205-07211,DBLP:journals/corr/abs-2210-15834}. The approach in ~\cite{DBLP:journals/taslp/LeiYWX22} has combined global emotion representation and local emotion representation of the fine-grained level, i.e., phoneme- or syllable-level, resulting in more natural expressive speech. However, in multilingual emotional speech synthesis, the relation between emotion and language is more complicated and needs careful consideration.

Significant progress has been made in the field of emotional speech synthesis in recent years~\cite{DBLP:journals/corr/abs-2206-14866,DBLP:journals/taslp/LeiYWX22,DBLP:journals/taslp/LiWXWX22,DBLP:journals/corr/abs-2110-04153}. When categorized emotion data is available for the target speaker, a straightforward approach is to model emotional expressions based on discrete emotion IDs~\cite{DBLP:conf/apsipa/AnLD17,DBLP:journals/corr/abs-1711-05447}. However, the resulting synthetic speech often exhibits over-averaged emotional expressions due to the discrete nature of emotion ID control. On the other hand, transfer learning approaches have been proposed, where a reference encoder is used to extract emotional representations from a reference signal, providing guidance for emotion synthesis. These emotion transfer approaches offer more flexibility and diversify the generated emotional speech~\cite{DBLP:conf/icml/Skerry-RyanBXWS18,DBLP:conf/interspeech/AkuzawaIM18,DBLP:conf/iscslp/LiYXX21}.

Emotion transfer is effective for generating emotional speech for speakers who only have neutral data, while it often faces a trade-off between speaker similarity and emotional expressiveness in synthetic speech. This trade-off results in either low speaker similarity or poor emotion similarity. To address this issue, disentangling these speech attributes and modeling them separately becomes necessary. Various schemes have been proposed for disentanglement. Some work implicitly decouples speech attributes in latent representations~\cite{DBLP:conf/interspeech/WangDYCLM21,DBLP:conf/interspeech/WangLZ0KM21}. Whitehill et al.~\cite{DBLP:conf/interspeech/WhitehillMMS20} use an unpaired training strategy and adversarial
cycle consistency scheme to disentangle emotion and speaker.  Li et al.~\cite{DBLP:journals/taslp/LiWXWX22} propose an emotion-disentangling module, which learns speaker-independence emotion embedding via an orthogonal loss with the speaker embedding. On the other hand, some studies~\cite{DBLP:conf/nips/ChoiLKLHL21,DBLP:conf/icassp/ChanQZH22} explicitly decouple speech attributes through bottleneck features, information perturbation, and other methods. Li et al.~\cite{DBLP:journals/spl/LeiYZXS22} learn emotion-related mel-spectrogram and speaker-related mel-spectrogram through information perturbation and generate emotionally expressive speech.



Considering human emotional expression's diverse and complex patterns, some research proposes to model emotional speech at multiple scales to capture rich emotional variations.~\cite{DBLP:journals/taslp/LeiYWX22,DBLP:journals/corr/abs-2205-07211,DBLP:journals/corr/abs-2210-15834}. For instance, the approach in~\cite{DBLP:journals/taslp/LeiYWX22} combines global emotion representation with local emotion representation at a fine-grained level, such as phoneme- or syllable-level, resulting in more natural and expressive speech. However, in the context of multilingual emotional speech synthesis, the entanglement between emotion and language becomes more complex and requires adequate consideration.


\subsection{Reference speech selection}

% Reference selection is usually critical to the naturalness and diversity of emotional expression in emotion transfer approaches. In practical non-parallel transfer applications, the textual contents of reference audio and generated audio are different at inference time, while they always remain the same at training time. This mismatch may lead to low speech naturalness~\cite{DBLP:conf/icml/ChangSKZT22,DBLP:journals/corr/abs-2303-04289} and inappropriate emotional expressions. Because there is no text-matched reference available during inference, how to select appropriate reference audio becomes a problem.

% Leveraging multiple references during training and inference, e.g., through simple averaging multiple reference embedding~\cite{DBLP:conf/icassp/GongWLZD22}, is straightforward to alleviate the mismatch problem. Recently, a more fancy approach based on context to select reference embedding has been proposed. ProsodySpeech~\cite{DBLP:conf/icassp/YiHPWX22} uses a Prosody Distributor to select references at the phone level with the help of an attention mechanism. Inspired by Contrastive Language-Image Pre-training (CLIP)~\cite{DBLP:conf/icml/RadfordKHRGASAM21}, CALM~\cite{DBLP:conf/interspeech/MengL0LSXSZM22} uses a Contrastive Acoustic-Linguistic Module to select reference speeches based on the input text on the Chinese corpus. 
% However, these approaches have not considered emotional reference; as discussed in the emotional speech synthesis section, human emotion is very complex. Moreover, the context content of multilingual scripts is more various, which is more challenging for contextual reference selection.

% However, the dynamic prosody patterns conveyed in multilingual speech are more complicated and should be modeled accurately enough to match the various linguistic content. 
%Therefore, a more accurate approach is using semantic similarity, which is utilized in natural language processing to select an appropriate answer under given questions~\cite{DBLP:conf/coling/Lan018,DBLP:journals/kbs/CherguiBG19}. Moreover, only leveraging the semantic content but ignoring acoustic content is not enough, 

The selection of appropriate reference speech plays a crucial role in ensuring the naturalness and diversity of emotional expression in emotion transfer-based TTS approaches. In practical non-parallel transfer applications, the textual contents of reference audio are different from that of generated speech during inference, while they remain the same during training. This mismatch can result in degraded speech naturalness~\cite{DBLP:conf/icml/ChangSKZT22,DBLP:journals/corr/abs-2303-04289} and inappropriate emotional expressions. Since there is no text-matched reference available during inference, the problem of selecting suitable reference audio becomes challenging.

One straightforward approach to address the mismatch problem is leveraging multiple references during training and inference, such as through simple averaging embeddings of multiple references~\cite {DBLP:conf/icassp/GongWLZD22}. Recently, more sophisticated approaches based on context have been proposed for reference selection. For instance, ProsodySpeech~\cite{DBLP:conf/icassp/YiHPWX22} utilizes a Prosody Distributor that employs an attention mechanism to select references at the phone level. Inspired by Contrastive Language-Image Pre-training (CLIP)~\cite{DBLP:conf/icml/RadfordKHRGASAM21}, CALM~\cite{DBLP:conf/interspeech/MengL0LSXSZM22} incorporates a Contrastive Acoustic-Linguistic Module to select reference speeches based on the input text.

However, as human emotions and the contextual content of multilingual scripts are highly complicated, addressing emotional reference selection and accounting for the diverse contextual content in multilingual scenarios remain essential research topics.

\section{Methodology}
\label{sc:method}

This section first gives an overview of METTS, followed by the motivation and design of each module. The training pipeline will also be introduced in this section.

\begin{figure*}[htb]
\centering
\begin{minipage}[b]{0.8\linewidth}
    \centerline
  \centerline{\includegraphics[width=\textwidth]{pipeline.drawio.pdf}}
\end{minipage}
\caption{The architecture of METTS.}
\label{fig_1}
\end{figure*}


\subsection{Overview}
\label{sc:Overview}

The proposed framework is built on a multilingual text processing front-end, which supports both Chinese and English. The front end encodes Chinese text input into labels of phonemes, tones, word boundaries, and prosodic boundaries while encoding English text input into labels of phonemes. Note that the Chinese phoneme set is based on Pinyin, while English is based on CMU-Dict. Therefore, we merge the Chinese and English phoneme sets for unified textual inputs of the bilingual TTS system.

As shown in Figure~\ref{fig_1}, the backbone of METTS is based on DelightfulTTS~\cite{DBLP:journals/corr/abs-2110-12612}, which consists of a text encoder, a variance adaptor, and a mel-spectrogram decoder. Notably, the improved Conformer~\cite{DBLP:conf/interspeech/GulatiQCPZYHWZW20} structure better model local and global dependency of mel-spectrogram, which has led DelightfulTTS to win the Blizzard Challenge 2021.
In general, METTS updates DelightfulTTS with \textit{multi-scale emotion modeling} to achieve natural multilingual emotional TTS by both emotion reference (METTS-REF) and ID (METTS-ID) as the control signal. Specifically, the coarse-grained emotion embedding is provided by a GST layer or an emotion matcher, while the fine-grained emotion embedding is obtained from the CVAE module. 
Moreover, a perturb module is introduced to distort the speaker timbre of the reference signal for decoupling timbre from speech. 
The speaker embedding is obtained through a lookup table and is added to the output of the variance adaptor, which is then fed into the mel-spectrogram decoder to synthesize the final mel-spectrogram.


\subsection{Multi-scale emotion modeling}
\label{sc:Multi-scale emotion modeling}

In general, the emotional expressions of multi-lingual speech could be factorized to the shared prosody pattern and distinct fine details of prosody due to the different manners of pronunciation. METTS utilizes the Global Style Tokens (GST) and Conditional Variational Autoencoder (CVAE) modules to establish coarse-grained language-agnostic and fine-grained language-specific emotional representations. 

The GST module employs a reference encoder to encode mel-spectrogram into a hidden representation and utilizes multi-head attention to calculate the global emotional style tokens. Notably, L2 normalization is applied to the global emotional representation, eliminating magnitude-related information that may vary across languages and speakers. This normalization improves the generalization ability of the model, allowing for emotion embedding control based solely on angular information geometrically~\cite{DBLP:conf/smc/KimLLJL21}. By mapping the emotional representation of different languages to the same global tokens, the GST module forms the language-agnostic representation.

The CVAE module focuses on learning fine-grained emotion expression from the mel-spectrogram, with text and coarse-grained emotion conditions. It utilizes a conformer block and a GRU layer to extract frame-level emotion embeddings, which are then downsampled to the phoneme level based on duration. These phoneme-level embeddings are used to derive the mean and variance of the distribution of phoneme-level prosody. Additionally, taking inspiration from VITS~\cite{DBLP:conf/icml/KimKS21}, a fine-grained predictor is designed to predict the distribution of fine-grained emotion from text. To improve the expressiveness of the predicted distribution, a normalizing flow technique is employed, enabling an invertible transformation from a simple distribution to a more complex one. By learning fine-grained emotional representations that are consistent with the text, the CVAE module forms the language-specific representation.

To ensure that the extracted multi-scale representations are relevant to emotions, even for training data without emotion annotations, a semi-supervised strategy is employed, which includes an emotion classifier. Specifically, only the embeddings of annotated audio are used to supervise the emotion classifier for both coarse- and fine-grained representations. The audio without emotion annotations is not involved to optimize the emotion classifiers and is utilized to train the acoustic model by the extracted embeddings. Furthermore, the frame and phoneme emotion embeddings are processed through a GRU layer to extract a single vector, which is then used as input to the emotion classifier.

\subsection{Speaker disentanglement based on information perturbation}

In our multilingual emotional speech synthesis setup, where each speaker is mono-lingual and only some speakers in the training set have emotional speech data, the entanglement of speaker timbre with emotion and language poses a challenge, resulting in synthetic speech with low speaker similarity and unusual emotional expression and pronunciation.

To address this issue more comprehensively, we adopt a pre-processing step that utilizes a signal perturbation module to remove the speaker timbre information. Specifically, we apply a dynamic \textit{formant} shift to the mel-spectrogram of the reference speech. Speech formants are primarily determined by the size, shape, and position of the vocal tract, which are highly specific to each speaker and represent their vocal identity~\cite{DBLP:journals/taslp/YooLY15}. By performing a formant shift function, denoted as $fs$, on the original waveform $Wave$, we obtain a speaker-independent signal denoted as $\widetilde{Wave} = fs (Wave)$.

Subsequently, we extract the mel-spectrogram of the perturbed wave, denoted as $\widetilde{Mel}$, which serves as the input for emotion representations extraction. The perturbation module perturbs the timbre of the recordings at a random scale by each step during training, allowing the GST and CVAE modules to learn a speaker-independent representation and effectively disentangle the speaker's timbre from speech.

\begin{figure*}[htb]

\begin{minipage}[b]{\linewidth}
    \centering
    \includegraphics[width=0.7\textwidth]{emotion_matcher.drawio1.pdf}
\end{minipage}
\caption{The architecture of emotion matcher.}
\label{fig_2}

\end{figure*}

\subsection{VQ based emotion matcher}

%During inference, the utilization of different coarse-grained language-agnostic emotion representations extracted from different references leads to variations in synthetic speech. 
During inference, different reference usually lead to different emotional expression of the synthetic speech. The selection of an appropriate reference is crucial for achieving natural speech and accurate emotional expression. Therefore, we propose an emotion matcher that automatically matches the most suitable reference embedding based on the textual content. Figure~\ref{fig_2} illustrates the architecture of the emotion matcher, which takes the text encoder output and the language ID as input and generates the optimal reference embedding, facilitating reference-free inference in METTS.

Directly modeling the complex relationship between bilingual textual representation and emotional representation is quite challenging, so we employ VQ to quantize the coarse-grained emotion representation, forming a reference pool. Subsequently, we predict the correct codebook from the reference pool for the current utterance. This transforms the intricate regression task into a simpler classifier task, simplifying the modeling process.


Specifically, in our approach, we begin by extracting the coarse-grained emotional representation and predicted emotion ID for all training audio samples using the GST layer and the emotion classifier. Subsequently, we apply VQ to quantize the coarse-grained emotional representation. To achieve this, we employ the k-means algorithm to obtain $N$ clusters for each emotion category, resulting in a total of $N \times M$ reference embeddings ($M$ is the number of emotion categories), which form the reference pool.

To select the appropriate reference from the pool, the emotion matcher utilizes a multilayer perceptron (MLP) to generate a text-emotion vector. This vector captures the contextual information related to emotion by taking the text encoder output and emotion ID as inputs. Using the text-emotion vector and the embedding-candidate pool, we calculate the correlation coefficient (CC) matrix between them. The calculation of CC is similar to the process of Scaled Dot-Product Attention~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} and is defined as:

\begin{equation}
CC(V_{t},E_{c}) = \text{softmax}\left(\frac{V_{t}E_{c}^T}{\sqrt{d_{E_C}}}\right),
\end{equation}
where $V_t$ and $E_c$ represent the text-emotion vector and embedding candidates, respectively, and $d_{E_C}$ denotes the dimension of the embedding candidates. The softmax function is applied to normalize the correlation coefficients. The embedding with the highest correlation coefficient is selected as the coarse-grained emotion representation for the TTS system.

To ensure that the selected embedding corresponds to the input text, a matcher classifier is introduced to supervise the CC matrix. It ensures that the embedding with the highest correlation coefficient is the cluster center corresponding to the input text.

\subsection{Training and fine-tuning}
For flexible control of the generated emotional expressions, we use pre-training and fine-tuning procedures to conduct multilingual emotional speech synthesis from a reference signal and manual emotion ID, respectively.

The training objective of METTS-REF is
% \vspace{-10pt}
\begin{equation}
\begin{aligned}  
\mathcal{L}_{\mathrm{pretrain}} = & 0.05 * \mathcal{L}_{\mathrm{kl}} +  \mathcal{L}_{\mathrm{prosody}} + \\
& 0.1 * \mathcal{L}_{\mathrm{emo}} + \mathcal{L}_{\mathrm{ssim}}+  \mathcal{L}_{\mathrm{iter}},
\end{aligned}\label{eq:eq1}
\end{equation}
where $\mathcal{L}_{\mathrm{prosody}}$ is the L1 loss between the predicted pitch/energy/duration and the ground-truth pitch/energy/duration, $\mathcal{L}_{\mathrm{emo}}$ is the semi-supervised crossentropy loss of emotion classifier. $\mathcal{L}_{\mathrm{kl}}$ is the KL divergence to predict the phoneme-level emotion distribution from text encoder output, and $\mathcal{L}_{\mathrm{iter}}$ is the sum of mel-spectrogram L1 loss between the predicted and ground-truth mel-spectrogram in each Conformer block. Moreover, we use structural similarity $\mathcal{L}_{\mathrm{ssim}}$ ~\cite{DBLP:journals/tip/WangBSS04} to measure the similarity between predicted and ground-truth mel-spectrogram in the final Conformer block.

The purpose of fine-tuning is to support METTS-ID. 
During fine-tuning, we use the ground-truth clustering center as the coarse-grained emotion representation for the TTS model and jointly optimize the emotion matcher. To stabilize the joint training, we freeze the GST-layer and emotion classifier as a discriminator to distinguish the emotion category of the generated mel-spectrogram.

The fine-tuning objective is
% \vspace{-8pt}
\begin{equation}   
\mathcal{L}_{\mathrm{finetune}} = \mathcal{L}_{\mathrm{match}} + \mathcal{L}_{\mathrm{disc}} + \mathcal{L}_{\mathrm{base^{'}}},
\end{equation}
where $\mathcal{L}_{\mathrm{match}}$ is the cross entropy loss between the selected clustering centre and the actual clustering centre in the emotion matcher, $\mathcal{L}_{\mathrm{disc}}$ is the emotion classification loss of the predicted mel-spectrogram taking GST layer as an discriminator, and $\mathcal{L}_{\mathrm{base^{'}}}$ means removing $\mathcal{L}_{\mathrm{emo}}$ from the pre-trained model objectives. 

\begin{table*}[h]
\centering
\caption{Dataset for the multilingual emotional TTS.}
\setlength{\tabcolsep}{3.0mm}
\label{tab:data}
\begin{tabular}{c|c|ccccccc|c}
\toprule
\multirow{2}{*}{Corpus}  & \multirow{2}{*}{Language} & \multicolumn{7}{c|}{Emotion (sentences)}   & \multirow{2}{*}{ Usage} \\
 &     & Neutral & Happy  & Surprise &Sadness & Angry & Disgust & Fear &   \\ \midrule
CN1        & Chinese  &5k    &0.5k   &0.5k     &0.5k    &0.5k   &0.5k   &0.5k       &Training\&Evaluation               \\
CN1     & Chinese  &5k   &2k    &2k    &2k   &2k  &2k  &2k     &Training\&Evaluation   \\
EN1       & English  &10k   &-   &-     &-    &-   &-   &-        &Training\&Evaluation   \\ 
EN2      & English  &10k   &-    &-     &-    &-   &-   &-       &Training\&Evaluation   \\ \bottomrule
\end{tabular}
\end{table*}

\section{Experimental Setups}
\label{sc:experiments}

This section introduces the database configuration, training setups, compared methods, and evaluation methods.

\subsection{Dataset} 
\label{sc:database}

To assess the performance of METTS, we conduct a series of experiments on Chinese and English datasets, as shown in Table~\ref{tab:data}. These two languages have tremendous pronunciation differences, which poses a challenge for multilingual speech synthesis. The Chinese dataset includes audio clips from two female speakers, denoted as CN1 and CN2, expressing six types of emotions~(anger, fear, happiness, sadness, surprise, and neutral). The total number of audio clips was 22,205, approximately 21 hours of audio in sum. The English dataset includes audio clips from two female speakers, EN1 and EN2, for 19,676 audio clips, approximately 20 hours. There is no apparent emotional expression in English datasets. % Its reading style is neutral in general, while some recordings may inevitably have slight expressions according to the semantic context of the text.
All data are studio-quality recorded at 48KHz.


\subsection{Training setups}
\label{exset}

For all texts, the TTS front end encodes Chinese text input into phonemes, tones, word boundaries, and prosodic boundaries while decoding English text input into labels of phonemes. We down-sample all the audios into 24k Hz and set the frame and hop sizes to 1200 and 300, respectively, when extracting optional auxiliary acoustic features like pitch and mel-spectrogram. The auxiliary pitch and energy contour is extracted through WORLD~\cite{DBLP:journals/ieicet/MoriseYO16}, and the implementation of formant shifting is achieved by Praat, following the NANSY~\cite{DBLP:conf/nips/ChoiLKLHL21} model. The phoneme duration is obtained through an HMM-based force alignment model~\cite{Sjlander2003AnHS}.

METTS takes DelightfulTTS~\cite{DBLP:journals/corr/abs-2110-12612} as the backbone, which consists of an encoder and decoder, both containing six conformer blocks. The dimensions of the emotion embedding and speaker embedding are set to 384. The CVAE module uses the Flow setting from VITS~\cite{DBLP:conf/icml/KimKS21}, and the dimension of the fine-grained emotion embedding is set to 16. The multi-layer perceptron (MLP) consists of one conformer block layer, six two-dimensional convolution layers, and one gated recurrent unit (GRU) layer, which outputs a 384-dimensional vector. The number of clusters in the k-means algorithm $N$ is set to 64. All classifiers have the same structure that consists of 3 fully connected layers with the Relu activation function.

All models are trained up to 400k steps on two 2080Ti GPUs with a batch size of 12 and use a MelGAN~\cite{DBLP:conf/nips/KumarKBGTSBBC19} vocoder to convert the generated mel-spectrogram into waveforms.


\subsection{Comparison methods}

As this work is the first attempt, to the best of our knowledge, to synthesize foreign emotional speech through emotion transfer from reference speech or directly based on emotion ID, there are no existing methods directly comparable to our proposed approach. However, we compare our proposed METTS with the most relevant and recent methods in the field to provide a fair evaluation.
To ensure fairness in the comparison, we implement the following comparison models on the delightful TTS model backbone and maintain identical training setups.

\begin{itemize}
  \item
  \textbf{CET}~\cite{DBLP:journals/corr/abs-2110-04153} is a powerful Cross-speaker Emotion Transfer speech synthesis system, which defines several emotion tokens that are trained to be highly correlated with corresponding emotions by a semi-supervised training strategy. Speaker condition layer normalization is implemented to eliminate the down-gradation to the timbre similarity for cross-speaker emotion transfer. During inference, the model transfers emotion from a reference mel-spectrogram to the synthetic speech.
  \item
  \textbf{M3}~\cite{DBLP:conf/interspeech/ShangHZZ021} is a Multi-speaker, Multi-style, and Multi-lingual text-to-speech system, which utilizes a speaker conditional variational encoder and conducts adversarial speaker training by the gradient reversal layer. Moreover, the model uses a Mixture Density Network (MDN) for mapping text and the extracted style vectors for each speaker. In inference time, the model predicts emotion representation according to emotion ID and text to synthesize speech.
  \item
  \textbf{METTS-REF} is the proposed model that transfers emotion from a reference mel-spectrogram to synthesize speech. 
  \item
  \textbf{METTS-ID} is the proposed model that automatically matches the most suitable reference embedding according to the input text and emotion ID to synthesize speech.
\end{itemize}

\begin{table*}[htb]
\centering
\caption{Results of subjective evaluation with 95$\%$ confidence interval for Chinese speakers.}
\label{tab_1}
\begin{tabular}{@{}c|ccc|ccc@{}}
\toprule
                             & \multicolumn{3}{c|}{Chinese Text}                            & \multicolumn{3}{c}{English Text}                      \\ \midrule
Model                        & Naturalness & Speaker Similarity & Emotion Similarity & Naturalness & Speaker Similarity & Emotion Similarity \\ \midrule
METTS-REF    & \textbf{4.11±0.12} & \textbf{3.94±0.16} & \textbf{4.12±0.14} & 4.00±0.11 & \textbf{3.94±0.12} & \textbf{3.44±0.22} \\
METTS-ID     & 4.07±0.13          & 3.88±0.16          & 3.95±0.13          & \textbf{4.06±0.18}          & 3.77±0.20          & 3.24±0.22          \\
\textbf{CET}~\cite{DBLP:journals/corr/abs-2110-04153}      & 3.65±0.14          & 3.69±0.12          & 4.00±0.11          & 3.01±0.19          & 3.35±0.14          & 3.39±0.16          \\
M3~\cite{DBLP:conf/interspeech/ShangHZZ021}     & 2.69±0.19          & 3.35±0.15          & 3.21±0.18          & 2.49±0.23          & 3.46±0.16          & 2.97±0.16          \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[htb]
\centering
\caption{Results of subjective evaluation with 95$\%$ confidence interval for English speakers.}
\label{tab_2}
\begin{tabular}{@{}c|ccc|ccc@{}}
\toprule
                             & \multicolumn{3}{c|}{Chinese Text}                            & \multicolumn{3}{c}{English Text}                      \\ \midrule
Model                        & Naturalness & Speaker Similarity & Emotion Similarity & Naturalness & Speaker Similarity & Emotion Similarity \\ \midrule
METTS-REF    & 3.91±0.14          & \textbf{3.68±0.18} & 3.71±0.15          & 3.95±0.14          & \textbf{3.82±0.16} & \textbf{3.44±0.19} \\
METTS-ID     & \textbf{4.02±0.15} & 3.57±21            & \textbf{3.73±0.17} & \textbf{4.05±0.18} & 3.74±0.18          & 3.26±0.14          \\
\textbf{CET}~\cite{DBLP:journals/corr/abs-2110-04153}      & 3.08±0.16          & 2.88±0.17          & 3.41±0.16          & 2.89±0.12          & 3.33±0.15          & 3.21±0.20          \\
M3~\cite{DBLP:conf/interspeech/ShangHZZ021}     & 2.72±0.15          & 3.17±0.15          & 3.01±0.18          & 2.41±0.19          & 3.24±01.5          & 2.81±0.18          \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Evaluation metrics} 
\label{sc:evaluation}

% To evaluate the performance of the benchmark systems, we prepare a set of forty English texts and forty Chinese texts as test sets. We generate samples for each speaker and emotion category, resulting in a total of 1,920 samples (2 languages $\times$ 40 texts $\times$ 4 speakers $\times$ 6 emotions) for evaluation. As for those models that  transfer emotion from a reference mel-spectrogram to synthesize speech, we provide randomly selected mel-spectrograms from CN\_spk1 as reference. As for those models that synthesize speech based on emotion ID, we provide emotion ID. 

To evaluate the performance of the benchmark systems, we conduct a comprehensive set of evaluation methods. We prepare two test sets consisting of forty English texts and forty Chinese texts. For each speaker and emotion category, we generate samples, resulting in a total of 1,920 samples (2 languages $\times$ 40 texts $\times$ 4 speakers $\times$ 6 emotions) for evaluation. For models that transfer emotion from a reference mel-spectrogram, we provide randomly selected mel-spectrograms from CN\_spk1 as the reference. For models that synthesize speech based on emotion ID, we provide the corresponding emotion ID as input.

For subjective evaluation, we conducted two types of human perceptual rating experiments.  A total of twenty-two volunteers with basic English skills participate in these experiments. Mean Opinion Score (MOS)~\cite{shang2021incorporating} is used to evaluate the naturalness of the synthetic speech. Participants are asked to rate the speech on a scale ranging from 1 to 5, reflecting the influence of foreign accents and emotion on naturalness. The rating criteria are as follows: bad = 1, poor = 2, fair = 3, good = 4, great = 5, with 0.5-point increments. Similarity Mean Opinion Scores (SMOS)~\cite{Li2021ControllableCE} is adopted to subjectively evaluate the synthetic speech from two aspects: emotion similarity and speaker similarity. Participants are asked to rate the speech's similarity to a given emotional reference and the similarity to the reference of the target speaker. The rating scale and criteria are the same as those used in the MOS evaluation.

For objective evaluation, we measure speaker cosine similarity, character error rate (CER), and word error rate (WER) for the synthetic audio. To measure speaker cosine similarity, we train an ECAPA-TDNN~\cite{DBLP:conf/interspeech/DesplanquesTD20} model trained on 3,300 hours of Mandarin speech and 2,700 hours of English speech from 18,083 speakers to extract x-vectors. We extract the averaged x-vector of all utterances for each English speaker and six averaged x-vectors for each emotion category of the Chinese speaker. We then extract the x-vector of the synthetic audio and calculate the cosine distance. A higher cosine similarity indicates a more similar speaker timbre. To evaluate CER and WER, we use an open-source model provided by the WeNet community~\cite{DBLP:conf/interspeech/YaoWWZYYPCXL21}, which uses the U2++ conformer architecture and is trained on 10,000 hours of open-source Gigaspeech English data~\cite{DBLP:conf/interspeech/ChenCWDZWSPTZJK21} and 10,000 hours of open-source WeNet Mandarin data~\cite{DBLP:conf/icassp/ZhangLGSYXXBCZW22}, respectively. A higher CER or WER indicates less accurate pronunciation.

%\vspace{-0.2cm}
\section{Experimental results}
\label{sc:results}

This section evaluates the performance of each system to produce bilingual emotional speech for Chinese and English speakers. The comparison between METTS and other methods is presented and discussed.


\subsection{Subjective evaluation}

We initially conducts a subjective evaluation to assess the performance of the generated multilingual emotional speech in terms of speech naturalness, speaker similarity, and emotion similarity for both Chinese and English speakers. The evaluation results, as presented in Table~\ref{tab_1} and Table~\ref{tab_2}, demonstrate that the proposed METTS family consistently outperforms the baseline models across all evaluation metrics for both Chinese and English speakers. Notably, all models exhibit a performance degradation during cross-lingual emotional speech synthesis, indicating that the synthetic Chinese speech for English speakers generally has lower quality compared to that for Chinese speakers. Nevertheless, the proposed METTS family demonstrates relatively minor degradation in performance during cross-lingual emotional speech synthesis, suggesting its capability to generate natural and fluent foreign speech for a given target speaker.

%The performance of different models will be analyzed in detail.
Comparing the different models in the METTS family, METTS-REF achieves the highest speaker and emotion similarity scores, indicating its effectiveness in transferring emotions from reference to synthetic speech. On the other hand, METTS-ID achieves almost the highest naturalness score and comparable emotion similarity to METTS-REF. This result validates the efficacy of the emotion matcher module in accurately matching a suitable reference embedding to synthesize more natural speech. Furthermore, there are two exceptional cases worth mentioning. In Table~\ref{tab_1}, METTS-REF achieves the highest naturalness score in synthesizing Chinese emotional speech for Chinese speakers, indicating that intra-lingual emotion expressions of different speakers are similar. In Table~\ref{tab_2}, METTS-ID obtains the highest emotion similarity score in synthesizing Chinese emotional speech for English speakers, which suggests that the coarse-grained emotion embedding provided by the emotion matcher module is close to that of the emotion encoder for English speakers in this particular condition.


CET demonstrates similar emotion similarity to METTS-REF under specific test conditions, indicating its powerful ability in emotion transfer. However, CET is primarily designed for inter-language emotion transfer and relies on a single-scale emotion representation, which is hard to capture the diverse emotional expressions across different languages. As a result, the synthetic speech may exhibit a heavy accent. Therefore, CET receives lower scores in naturalness and speaker similarity evaluations. In contrast, our proposed METTS model incorporates multi-scale emotion modeling to capture both language-specific and language-agnostic emotional expressions, effectively avoiding the entanglement of accents with emotions.
Furthermore, M3 performs poorly across all evaluation metrics. M3 assumes a strong correlation between style coding and the speaker's attributes and content~\cite{DBLP:conf/interspeech/ShangHZZ021}, which leads to an entanglement between the speaker's timbre and emotion. Additionally, the domain adversarial training used in M3 for speaker timbre disentanglement is not stable~\cite{DBLP:conf/iclr/AcunaLZF22}. In contrast, our proposed model employs information perturbation to effectively remove the speaker's timbre, resulting in a more stable and practical approach.


\subsection{Objective evaluation}
\label{sc:emotrans}

To comprehensively evaluate the performance of our multilingual emotional TTS system, we conduct objective tests to measure speaker cosine similarity, character error rate (CER) for synthetic Chinese speech, and word error rate (WER) for synthetic English speech.

\begin{table}[htb!]
\centering
\caption{Results of objective evaluation for Chinese speakers.}
\label{tab_7}
\begin{tabular}{@{}c|cc|cc@{}}
\toprule
                             & \multicolumn{2}{c|}{Chinese Text} & \multicolumn{2}{c}{English Text} \\ \midrule
Model                        & Cosine Similarity                & CER          & Cosine Similarity             & WER            \\ \midrule
METTS-REF  & \textbf{0.813} & 0.48          & \textbf{0.753}  & 5.60                  \\
METTS-ID & 0.805          & 0.48          & 0.711  & \textbf{5.46} \\
\textbf{CET}~\cite{DBLP:journals/corr/abs-2110-04153}  & 0.726          & \textbf{0.35} & 0.638   & 12.65                 \\
M3~\cite{DBLP:conf/interspeech/ShangHZZ021}  & 0.754          & 11.02         & 0.673  & 55.32                 \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[htb!]
\centering
\caption{Results of objective evaluation for English speakers.}
\label{tab_8}
\begin{tabular}{@{}c|cc|cc@{}}
\toprule
                             & \multicolumn{2}{c|}{Chinese Text} & \multicolumn{2}{c}{English Text} \\ \midrule
Model                        & Cosine Similarity                & CER          & Cosine Similarity             & WER            \\ \midrule
METTS-REF   & \textbf{0.735}            & 1.38                           & 0.769 & 5.51                                              \\
METTS-ID    & 0.709            & 1.36                           & \textbf{0.786}  & \textbf{2.15}                           \\
\textbf{CET}~\cite{DBLP:journals/corr/abs-2110-04153}     & 0.659                     & \textbf{1.24}                  & 0.663                   & 8.05                           \\
M3~\cite{DBLP:conf/interspeech/ShangHZZ021}           & 0.671                     & 16.74                          & 0.704                       & 38.22                                      \\ \bottomrule
\end{tabular}
\end{table}

% As presented in Tables~\ref{tab_7} and Table~\ref{tab_8}, the objective test results confirm the findings of the subjective evaluation, indicating that inter-lingual speech synthesis outperforms cross-lingual speech synthesis. The METTS family achieves the highest speaker cosine similarity, providing evidence of the effectiveness of our approach in disentangling speaker timbre from emotion and language. Moreover, the METTS family obtains balanced and low CER and WER, indicating its stability in generating accurate and natural-sounding multilingual emotional speech.

The objective test results presented in Tables~\ref{tab_7} and Table~\ref{tab_8} confirm the observations from the subjective evaluation, highlighting the distinction between inter-lingual and cross-lingual speech synthesis. The METTS family achieves the highest speaker cosine similarity, demonstrating the effectiveness of our approach in disentangling speaker timbre from both emotion and language. Furthermore, the METTS family achieves lower CER and WER, indicating its stability in generating intelligent, natural-sounding multilingual emotional speech.

% CET gets the lowest CER, which indicates its ability in emotion transfer intra-lingual. However, the WER of CET is much higher as cross-lingual emotion transfer is significantly challenging, which aligns with the subjective evaluation results regarding naturalness. In addition, M3 fails to effectively address the accent-related challenges in multilingual emotional speech synthesis, leading to incorrect pronunciation and yielding the highest CER and WER scores. Moreover, the result of speaker cosine similarity suggests that information perturbation is better than SALN in CET and speaker adversarial training in M3 in speaker disentanglement of multilingual emotional speech synthesis.

It is worth noting that CET achieves the lowest Chinese CER, showcasing its ability in intra-lingual emotion transfer. However, the higher English WER of CET reflects the significant challenges of cross-lingual emotion transfer, which aligns with the subjective evaluation results concerning naturalness. Additionally, M3 fails to effectively address accent-related challenges in multilingual emotional speech synthesis, resulting in incorrect pronunciation and yielding the highest CER and WER. Furthermore, the results of speaker cosine similarity suggest that information perturbation for speaker timbre removal employed in our approach is more effective than the SALN method used in CET and the speaker adversarial training method in M3 in terms of speaker disentanglement in multilingual emotional speech synthesis.

\begin{table*}[htb]
\centering
\caption{Results of Ablation study with 95$\%$ confidence interval for Chinese speakers.}
\label{tab_9}
\begin{tabular}{@{}l|ccc|ccc@{}}
\toprule
                             & \multicolumn{3}{c|}{Chinese Text}                            & \multicolumn{3}{c}{English Text}                      \\ \midrule
Model                        & Naturalness & Speaker Similarity & Emotion Similarity & Naturalness & Speaker Similarity & Emotion Similarity \\ \midrule
METTS-REF     & \textbf{4.11±0.12} & \textbf{3.94±0.16} & 4.12±0.14 & \textbf{4.00±0.11}  & \textbf{3.94±0.12}  & 3.44±0.22 \\
\: - GST                 & 3.50±0.15 & 3.82±0.17 & 3.19±0.18  & 3.69±0.13 & 3.80±0.18  & 3.07±0.17 \\
\: - CVAE              & 3.95±0.12 & 3.81±0.16  & 3.88±0.12 & 3.43±0.14 & 3.82±0.13  & 3.39±0.17 \\
\: - Perturb & 4.02±0.12 & 3.87±0.15 &\textbf{4.19±0.14} & 3.45±0.17 & 3.55±0.16   & \textbf{3.58±0.21} \\ \bottomrule
\end{tabular}
\end{table*}

\begin{table*}[htb]
\centering
\caption{Results of Ablation study with 95$\%$ confidence interval for English speakers.}
\label{tab_10}
\begin{tabular}{@{}l|ccc|ccc@{}}
\toprule
                             & \multicolumn{3}{c|}{Chinese Text}                            & \multicolumn{3}{c}{English Text}                      \\ \midrule
Model                        & Naturalness & Speaker Similarity & Emotion Similarity & Naturalness & Speaker Similarity & Emotion Similarity \\ \midrule
METTS-REF     & \textbf{3.91±0.14} & \textbf{3.68±0.18} & 3.71±0.15 & \textbf{3.95±0.14}  & \textbf{3.82±0.16}  & 3.44±0.19 \\
\: - GST                & 3.75±0.19          & 3.52±0.21          & 3.24±0.21          & 3.73±0.19          & 3.64±0.21          & 3.17±0.18          \\
\: - CVAE             & 3.71±0.18          & 3.56±0.19          & 3.58±0.19          & 3.17±0.18          & 3.76±0.21          & 3.30±0.20          \\
\: - Perturb   & 3.85±0.21 & 3.19±0.27          & \textbf{3.82±0.19} & 3.50±0.22 & 3.26±0.29                  & \textbf{3.52±0.20} \\ \bottomrule
\end{tabular}
\end{table*}

\subsection{Visual analysis of emotional representation}

\begin{figure}[htb]
\begin{minipage}[b]{\linewidth}
\centering
\begin{subfigure}[b]{0.9\textwidth}
\centering
\includegraphics[width=\textwidth]{test7.pdf}
\caption{Colored by emotion.}
\vspace{+10pt}
\end{subfigure}
\begin{subfigure}[b]{0.9\textwidth}
\centering
\includegraphics[width=\textwidth]{test_spk1.pdf}
\caption{Colored by speaker.}
\end{subfigure}
\end{minipage}
\caption{T-SNE visualization of emotion embedding. The difference between (a) and (b) is that they are colored by different attributes.}
\label{fig_3}
\end{figure}

We further visualize 
We further visualize the coarse-grained emotional representation via T-SNE~\cite{Maaten2008VisualizingDU}. Specifically, we preserve 100 utterances per emotion in the Chinese training speech data and 600 in English.

Figure~\ref{fig_3}(a) presents the T-SNE visualization of the emotion embeddings for Chinese utterances, demonstrating clear clusters. This observation validates the effectiveness of our semi-supervised emotion classifier. However, in Figure~\ref{fig_3}(a), we notice that certain emotion embeddings of English utterances are intermixed with those of Chinese utterances. We hypothesize that the language-agnostic nature of the coarse-grained emotion representation enables it to capture subtle emotional expressions in English utterances, resulting in their clustering alongside the Chinese utterances. This intermixed phenomenon of coarse-grained emotion representation signifies the METTS family's ability to transfer emotions across languages.

To further explore the extent to which the emotional representation encompasses the speaker's timbre attribute, we color the T-SNE visualization based on speaker ID. Figure~\ref{fig_3}(b) illustrates that the emotion embeddings are not well clustered according to the speaker, providing evidence of the speaker's independence in the coarse-grained emotion representation. This finding reinforces the effectiveness of our approach in disentangling speaker characteristics and isolating them from the emotional representation.


% \subsection{Case Study}
% \label{sc:emotrans}

\section{Component analysis}
\label{component}

In Section~\ref{sc:results}, we demonstrate the excellent performance of METTS in both intra- and cross-lingual scenarios of emotional speech synthesis.
In this section, we aim to evaluate the effectiveness of each component by examining their impact on naturalness, speaker similarity, and emotion similarity.
Additionally, we analyze the influence of different values of clusters on the performance of METTS-ID.

\subsection{Ablation study of METTS-REF}

\begin{table*}[]
\centering
\caption{Results of different values of $N$ on model's performance with 95$\%$ confidence interval for Chinese speakers.}
\label{tab_11}
\begin{tabular}{@{}c|cccc|cccc@{}}
\toprule
   & \multicolumn{4}{c|}{Chinese Text}                                             & \multicolumn{4}{c}{English Text}                                              \\ \midrule
$N$  & Accuracy       & Naturalness        & Speaker Similarity & Emotion Similarity & Accuracy       & Naturalness        & Speaker Similarity & Emotion Similarity \\ \midrule
32 & \textbf{0.916} & 3.91±0.13          & 3.60±0.15          & 3.51±0.19          & \textbf{0.920} & 3.81±0.13          & 3.60±0.15          & 2.95±0.21          \\
64 & 0.854          & \textbf{4.07±0.13} & \textbf{3.88±0.16} & \textbf{3.95±0.13} & 0.875          & \textbf{4.06±0.18} & \textbf{3.77±0.20} & \textbf{3.24±0.22} \\
96 & 0.656          & 4.00±0.12          & 3.76±0.14          & 3.68±0.16          & 0.664          & 3.86±0.14          & 3.63±0.17          & 2.98±0.20          \\ \bottomrule
\end{tabular}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table*}[]
\centering
\caption{Results of different values of $N$ on model's performance with 95$\%$ confidence interval for English speakers.}
\label{tab_12}
\begin{tabular}{@{}c|cccc|cccc@{}}
\toprule
   & \multicolumn{4}{c|}{Chinese Text}                                             & \multicolumn{4}{c}{English Text}                                              \\ \midrule
$N$  & Accuracy       & Naturalness        & Speaker Similarity & Emotion Similarity & Accuracy       & Naturalness        & Speaker Similarity & Emotion Similarity \\ \midrule
32 & \textbf{0.916} & 3.80±0.17          & 3.62±0.17          & 3.57±0.19          & \textbf{0.920} & 3.76±0.18          & 3.65±0.18          & 3.21±0.17          \\
64 & 0.854          & \textbf{4.02±0.15} & 3.57±0.21          & \textbf{3.73±0.17} & 0.875          & \textbf{4.05±0.18} & \textbf{3.74±0.18} & \textbf{3.26±0.14} \\
96 & 0.656          & 3.90±0.20          & \textbf{3.72±0.15} & 3.64±0.17          & 0.664          & 3.74±0.16          & 3.52±0.18          & 3.10±0.22          \\ \bottomrule
\end{tabular}
\end{table*}
We conduct ablation studies where the GST module, CVAE module, and perturb module are removed individually. The corresponding results are presented in Table~\ref{tab_9} and Table~\ref{tab_10}, respectively.

The removal of the GST module significantly affects the control of global emotional expression in bilingual speech. Without GST, METTS fails to map emotional expressions of different languages to the same global token and provide global emotion conditions. As a result, there is a significant decrease in emotion similarity and noticeable declines in naturalness and speaker similarity. This highlights the crucial role of the coarse-grained language-agnostic emotion representation in our approach.

Furthermore, when the CVAE module is removed, there is a sharp decline in naturalness and a decrease in emotion similarity. This indicates that the fine-grained emotional representation learned by the CVAE module, which is consistent with the input text, not only enhances the emotional expression but also plays a vital role in addressing the foreign accent problem and improving the overall naturalness of the synthetic speech.

% Although the omission of information perturbation slightly increases emotion similarity in most test conditions, it significantly compromises naturalness and speaker similarity. This suggests a trade-off and substantial entanglement between speaker timbre, emotion, and language in multilingual emotional speech synthesis, as speaker timbre entangled with language leads to abnormal pronunciation and speaker timbre entangled emotion leads to high emotional expressiveness but low speaker similarity. The necessity of speaker disentanglement becomes apparent in order to achieve idiomatic pronunciation for each speaker.

Regarding the perturbation module, its omission slightly increased emotion similarity in most test conditions. However, it significantly compromised naturalness and speaker similarity. This trade-off suggests a substantial entanglement between speaker timbre, emotion, and language in multilingual emotional speech synthesis. Speaker timbre entangled with language may lead to abnormal pronunciation, while speaker timbre entangled with emotion may result in slightly high emotional expressiveness but low speaker similarity. Therefore, the necessity of speaker disentanglement becomes apparent to achieve idiomatic pronunciation and natural emotional expression for each speaker.


\subsection{Ablation study of METTS-ID}

Given the significance of the codebook size in Vector Quantization (VQ), we investigate the impact of different values of $N$ in the emotion matcher module on the performance of METTS-ID. Alongside evaluating naturalness, speaker similarity, and emotion similarity, we also examine the accuracy of the emotion matcher. For analysis, we retain 100 utterances per emotion in Chinese and 600 in English.

We first evaluate the accuracy of the emotion matcher by extracting the ground-truth cluster labels for each utterance and calculating the predicted accuracy. The results, presented in Table~\ref{tab_11} and Table~\ref{tab_12}, demonstrate that as the value of $N$ increases, there is an increase in the diversity of emotion embeddings. In contrast, the predicted accuracy of the emotion matcher gradually decreases. This indicates a complex trade-off between the diversity of emotion embeddings and the predicted accuracy of the emotion matcher. Notably, the predicted accuracy remains consistent across target speakers and languages, ensuring that METTS-ID can generate natural and emotionally expressive bilingual speech for each target speaker.
%Additionally, there is no significant difference between the Chinese and English text inputs, indicating the model's sound understanding of the contextual content in multilingual scripts.

Furthermore, as shown in Table~\ref{tab_11} and Table~\ref{tab_12}, the effect of $N$ on speaker similarity is negligible, while naturalness and emotion similarity achieve their highest scores when $N$ is set to 64. Therefore, considering the overall performance, we designate $N$ as 64 to strike a balance between predicted accuracy, naturalness, speaker similarity, and emotion similarity.


\section{Conclusion}
\label{sc:conclusion}

This paper proposes METTS for multilingual emotional speech synthesis, aiming at achieving natural and diverse bilingual emotional speech across speakers. First, we introduce multi-scale emotion modeling to learn emotional expressions from a language-agnostic emotion representation (coarse-grained) and a language-specific emotion representation (fine-grained), effectively addressing the foreign accent problem. Meanwhile, we leverage information perturbation to address the problem of speaker timbre coupling and obtain speaker-independent multi-scale emotion representation. Moreover, we design a VQ-based emotion matcher to construct an embedding-candidate pool and select appropriate references according to the input text and emotion category for better emotional diversity and the naturalness of synthetic speech. English-Chinese bilingual experiments show that METTS can synthesize expressive bilingual speech with natural emotion and native pronunciation for each mono-lingual speaker.

During our investigation of the multilingual emotional TTS system through cross-speaker cross-lingual emotion transfer, we have identified the need for further improvements in synthesizing English emotional speech for both Chinese and English speakers. This is mainly because the English training corpus is mainly neutral in our study. We believe that leveraging emotional English corpus to train METTS will effectively improve the expressiveness of English synthetic speech in multilingual emotional text-to-speech.

% In this study, we investigate the multilingual emotional TTS system through cross-speaker cross-lingual emotion transfer in the inter-gender situation (female to female in our experiments). As we know, cross-gender emotion transfer is even more challenging as the distinct characteristics in pitch and pronunciation fine-details in the spectrum. Our future research plans to focus on cross-gender emotion transfer in multilingual emotional TTS to address this limitation.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.




%\textbf{Comparison of Audio2Keypoint with various target independent audio-driven reenactment methods}

% \textbf{dense optical flow} a simple dense optical flow method Farmeback \cite{farneback2003two} is adopted to calculate the dense flow of the generated videos.







% use section* for acknowledgment
% \section*{Acknowledgment}


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
% \ifCLASSOPTIONcaptionsoff
%  \newpage
% \fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
\bibliography{mybibfile.bib}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

\end{document}
