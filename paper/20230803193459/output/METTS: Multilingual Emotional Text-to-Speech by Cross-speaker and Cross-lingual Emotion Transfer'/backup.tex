
%% bare_jrnl_comsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Communications Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal,comsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal,comsoc]{../sty/IEEEtran}


\usepackage[T1]{fontenc}% optional T1 font encoding
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{url}
\usepackage{bm}
\usepackage{xcolor}
% \usepackage{cite}
\urlstyle{same}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\cmmnt}[1]{}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
% Do NOT use the amsbsy package under comsoc mode as that feature is
% already built into the Times Math font (newtxmath, mathtime, etc.).
% 
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% Select a Times math font under comsoc mode or else one will automatically
% be selected for you at the document start. This is required as Communications
% Society journals use a Times, not Computer Modern, math font.
\usepackage[cmintegrals]{newtxmath}
% The freely available newtxmath package was written by Michael Sharpe and
% provides a feature rich Times math font. The cmintegrals option, which is
% the default under IEEEtran, is needed to get the correct style integral
% symbols used in Communications Society journals. Version 1.451, July 28,
% 2015 or later is recommended. Also, do *not* load the newtxtext.sty package
% as doing so would alter the main text font.
% http://www.ctan.org/pkg/newtx
%
% Alternatively, you can use the MathTime commercial fonts if you have them
% installed on your system:
%\usepackage{mtpro2}
%\usepackage{mt11p}
%\usepackage{mathtime}


%\usepackage{bm}
% The bm.sty package was written by David Carlisle and Frank Mittelbach.
% This package provides a \bm{} to produce bold math symbols.
% http://www.ctan.org/pkg/bm





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Synthesizing spoken descriptions of images}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

% \author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%         John~Doe,~\IEEEmembership{Fellow,~OSA,}
%         and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
% \thanks{M. Shell was with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
% \thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% \author{Xinsheng~Wang,
%         Tingting~Qiao,
%         Jihua~Zhu,
%         Alan~Hanjalic, ~\IEEEmembership{Fellow,~IEEE,}
%         Odette Scharenborg, ~\IEEEmembership{Senior Member,~IEEE}% <-this % stops a space
%         \thanks{Xinsheng Wang is with the School of Software Engineering, Xi’an Jiaotong University, Xi’an 710049, China, and also with the Multimedia
% Computing Group, Delft University of Technology, 2628 CD Delft, The
% Netherlands. Email: wangxinsheng@stu.xjtu.edu.cn}
%         \thanks{Tingting Qiao is with the College of Computer Science and Technology, Zhejiang University, Hangzhou 310007, China, and also with the Multimedia Computing Group, Delft University of Technology, 2628 CD Delft, The Netherlands. Email: qiaott@zju.edu.cn}
%         \thanks{Jihua Zhu is with the School of Software Engineering, Xi’an Jiaotong University, Xi’an 710049, China. Email: zhujh@xjtu.edu.cn}
%         \thanks{Alan Hanjalic and Odette Scharenborg are with the Multimedia Computing Group, Delft University of Technology, 2628 CD Delft, The Netherlands. Email: A.Hanjalic@tudelft.nl, O.E.Scharenborg@tudelft.nl.}
        
%         }

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!

%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Communications Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
% \begin{abstract}
% Text-based technologies, such as text translation from one language to another, and image captioning, are gaining popularity. However, approximately half of the world's languages are estimated to be lacking a commonly used written form. Consequently, these languages cannot benefit from text-based technologies. This paper presents 1) a new speech technology task, i.e., a speech-to-image generation (S2IG) framework which translates speech descriptions to photo-realistic images 2) without using any text information, thus allowing unwritten languages to potentially benefit from this technology. The proposed speech-to-image framework, referred to as S2IGAN, consists of a speech embedding network and a relation-supervised densely-stacked generative model. The speech embedding network learns speech embeddings with the supervision of corresponding visual information from images. The relation-supervised densely-stacked generative model synthesizes images, conditioned on the speech embeddings produced by the speech embedding network, that are semantically consistent with the corresponding spoken descriptions. Extensive experiments are conducted on four public benchmark databases: two databases that are commonly used in text-to-image generation tasks, i.e., CUB-200 and Oxford-102 for which we created synthesized speech descriptions, and two databases with natural speech descriptions which are often used in the field of cross-modal learning of speech and images, i.e., Flickr8k and Places. Results on these databases demonstrate the effectiveness of the proposed S2IGAN on synthesizing high-quality and semantically-consistent images from the speech signal, yielding a good performance and a solid baseline for the S2IG task.

% \end{abstract}

% Note that keywords are not normally used for peerreview papers.
% \begin{IEEEkeywords}
% Speech-to-image generation, multimodal modelling, speech embedding, adversarial learning.
% \end{IEEEkeywords}




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
\label{sc:Introduction}
\IEEEPARstart{A}{utomatically} describing visual scenes with natural language has great potential in many scenarios, e.g., for helping visually-impaired people interact with their surroundings. Recent years, many literatures \cite{xu2015show,chen2018boosted,li2019entangled,huang2019attention,cornia2020meshed,rennie2017self,chen2017show,keneshloo2019deep} have been published in the field of image captioning that aims to automatically synthesize textual descriptions of images. This task, inspired by the architecture of neural machine translation and benefited from the development of attention mechanism, has achieved impressive results. However, this textual language-based method avoid people who use unwritten languages from benefiting this image describing technology. In fact, a written form is inaccessible for nearly half of the world's languages, which are unwritten languages. Therefore, it is necessary to develop a technology to automatically describe visual scenes bypassing text, making speakers of any languages can benefit from the image captioning system.

Hasegawa-Johnson et al. \cite{hasegawa2017image2speech} first proposed the image-to-speech task that tries to synthesize spoken descriptions of images without using textual descriptions. In their method, the image-to-speech was decomposed into two stages. The first stage is to generate speech units, e.g., phonemes, with image as input. The second stage is to perform the phoneme-to-speech synthesis process. Different speech units, i.e., L1 phonemes transcribed by an ASR that trained with the same language, L2 phonemes transcribed by an ASR that trained with another language, and Pseudo phones that generated by an unsupervised acoustic unit discovery system. The L2 phonemes and Pseudo phones based methods make the image-to-speech system can be used for an unwritten language, while the L1 phonemes based method provides the convenient of comparisons for possible newly proposed methods due to that the L2 phonemes and Pseudo phones are highly depend on the unsupervised speech unit discovery method which is an important and independent topic to be investigated. 

This image-to-speech method \cite{hasegawa2017image2speech} provides a chance for people that using unwritten languages can benefit from the image describing system. However, the image-to-speech task, as a new task, still has many gaps to be filled. 

First, although the image-to-speech is a new task, the image-to-speech unit process shares the same idea as that in the text-based image captioning methods, both of which follow the basic structure of neural machine translation. Compared with the image-to-speech task, there are much more models have been proposed in the field of image captioning. However, there has not been any work to show whether those image captioning method can be used for image-to-speech synthesis, and how well they can perform in the image-to-speech task.

Besides, in the original image-to-speech paper \cite{hasegawa2017image2speech}, the evaluation was performed for the speech unit synthesis process, and the BLEU score and unit error rates were adopted as the evaluation metrics. However, no methods or experiments have been proposed to show how to appropriately evaluate the image-to-speech unit process, i.e., which evaluation metrics are suitable to be used to evaluate the image-to-speech unit methods.

Moreover, the performance of this speech unit based method depends on the quality of speech unit discovery. The L1 phonemes that transcribed from the well-trained same language ASR can bring good performance for the image-to-speech system, but they cannot be used for unwritten languages. The L2 phonemes and Pseudo phones can be used for unwritten languages, but how to get good quality L2 phonemes and Pseudo phones are still quite challenging tasks, and non-L1 phonemes adopted in \cite{hasegawa2017image2speech} showed much worse performance compared to the L1 phoneme based method. 

In this paper, we try to fill these gaps. In order to show whether models designed for image captioning task can be used in the speech unit-based image-to-speech system, and how well they perform in the image-to-speech system, we implemented several representative image captioning models in the image-to-speech system. The image-to-speech unit model proposed in \cite{hasegawa2017image2speech} was re-implemented to work as the baseline that compared to those image captioning methods.

To give an evaluation of how effective different evaluation metrics on the image-to-speech unit are, we conduct a human rating experiment on the results produced by several different models. Then, the human ratings were correlated with different evaluation metrics to show which metric has the best consistency with the human ratings. So that we can determine which evaluation metric could be the best choice to evaluate the image-to-speech unit task.

Last, considering that L1 phonemes cannot be used for the unwritten languages, while L2 phones and Pseudo phones perform very worse on the image-to-speech task, and developing an unsupervised system to get good enough speech units are still quite challenging. In order to make the image-to-speech system able to bypass the dependency on both text and speech units, we proposed an end-to-end method which can synthesize spoken descriptions directly from images.

Preliminary works were presented in \cite{van2020evaluating,wang2020show}, in which we investigated how an image-to-phoneme system could be evaluated objectively on the basis of the re-implemented model of \cite{hasegawa2017image2speech} and proposed an end-to-end image-to-speech model, respectively. In this paper, more models proposed for the image captioning task were re-implemented to the image-to-speech task, and the human rating experiments to investigate how to objectively evaluate the image-to-phoneme system were performed on more methods. The contributions of this work are as follows:

\begin{itemize}
   \item Experiments of the image-to-phoneme task on the basis of various image captioning models proved that image captioning methods can have a good performance on the image-to-phoneme task.
    \item Analysis of the correlation between various evaluation metrics and human rating gave an insight into how to correctly evaluate the image-to-phoneme system.
    \item An end-to-end image-to-speech method, for the first time, was proposed, which demonstrates that synthesizing spoken descriptions for images while bypassing text and phonemes is feasible.  
\end{itemize}

The rest of the paper is organized as follows: Section \ref{sc:related works} reviews some related works, including image captioning and visual-speech multi-modal learning. Section \ref{sc:Image-to-phoneme} introduces several image captioning models that were re-implemented in the image-to-speech task. Several evaluation metrics and the human rating method are also introduced in this section. Section \ref{sc:End2end model} describes the proposed end-to-end method. Section \ref{sc:results} presents the results both of the image-to-phoneme task and the end-to-end method. Section \ref{sc:discussion} discusses the limitation of the current image-to-speech methods. Finally, Section \ref{sc:conclusion} concludes this paper. 

\section{Related works}
\label{sc:related works}

As we would like to investigate whether image captioning methods can be used in the image-to-speech system, the related works on image captioning will be reviewed in this section. Besides, the image-to-speech is a visual-speech cross-modal task and the related works on the visual-speech cross-modal task will also be reviewed.



\subsection{Image captioning}
To do
% Earlier image captioning approaches were based on the outputs of an object detector or attribute predictor to caption templates \cite{socher2010connecting,yao2010i2t}. Inspired by the development of neural machine translation, neural-based architectures have been the main framework in the the recent image captioning approaches.  

% **

% In \cite{xu2015show}, the spatial attention mechanism on CNN feature map is used to incorporate visual context.

% In \cite{chen2017sca}, spatial and channel-wise attention model is proposed.

% In \cite{lu2017knowing}, an adaptive attention mechanism is introduced to decide when to activate the visual attention.



% Inspired by the development of neural machine translation, attention mechanisms have been widely used current encoder/decoder frameworks for visual captioning and achieved impressive results. In such a framework for image captioning, an image is first  encoded to a set of feature vectors via a CNN based network and then decoded to words via an RNN based network, where the attention mechanism guides the decoding process by generating a weighted average over the extracted feature vectors for each time step.



% On the training side, the standard training methods are based on a time-wise cross-entropy, a notable achievement has been mad with the introduction of Reinforcement Learning, which enable the use of non-differentiable caption metrics as optimization objectives \cite{ranzato2015sequence,liu2017improved,rennie2017self}
% Pre-train the model with a word-level cross-entropy loss and finetune the sequence generation using reinforcement learning.




% The Transformer comprises an encoder made of stack of self-attention and feed-forward layers, and a decoder which uses self-attention on words and cross-attetnion over the output of the lass encoder layer. 

% Transformer: Heerdade et al. \cite{herdade2019image} used the Transformer architecture for image captioning and incorporated geometric relations between detected input objects. In particular, they computed an additional geometric weight between object pairs which is used to scale attention weights. 

% Li et al. \cite{li2019entangled} used the Transformer in a model that exploits visual information and additional semantic knowledge given by an external tagger.

% Huang et al. \cite{huang2019attention} introduced an extension of the attention operator in which the final attended information is weighted by a gate guided by the context. In their approach, a Transformer-like encoder was paired with an LSTM decoder.


\subsection{Visual-speech cross-modal learning}
To do

\section{Image-to-phoneme}
\label{sc:Image-to-phoneme}
Introduction of this section: To do
\subsection{Re-implementation of the image-to-phoneme model}
Following the original paper of the image-to-phoneme \cite{hasegawa2017image2speech}, the eXtensible Neural Machine Translation Toolkit (XNMT) \cite{neubig2018xnmt} was adopted to re-implement the image-to-phoneme model. The image-to-phoneme model is an attention-guided encoder-decoder architecture. The encoder takes image features as input, and the decoder outputs the predicted phoneme sequence. The encoder uses 3 layers pyramidal LSTM with 128 units. The attender uses a multi-layer perceptron with a state dimension of 512 and a hidden dimension of 128. The decoder is a 3 layers LSTM with 512 units followed by a multi-layer perceptron with a hidden dimension of 1024 working as a transformation between the outputs of LSTM and a final softmax layer. Compared to the original image-to-phoneme model \cite{hasegawa2017image2speech}, we made slight changes that an increase of the encoder layers from 1 to 3 and an increase of the attender state dimension from 128 to 512, by doing which the performance can achieve an increase. For convenience, this re-implemented image-to-phoneme model is referred to as \textbf{I2P} hereafter.  Image features: To do

\subsection{Image captioning methods}

To do: Why choose those methods

\textbf{SAT} \cite{xu2015show} : To do

\textbf{Att2in} \cite{rennie2017self}: To do

\textbf{Updown model} \cite{anderson2018bottom} combines the bottom-up and top-down attention that enables attention to be calculated at the level of objects and other salient image regions. The "bottom-up" refers to the purely visual feed-forward attention mechanisms and the "top-down" refers to attention mechanisms driven by non-visual or task-specific context. In this model, the bottom-up mechanism is based on the faster R-CNN to detect the interesting regions of the images, and the top-down mechanism is to determine the feature weights of different image regions.

\textbf{Attention on Attention model (AoANet)} \cite{huang2019attention} takes a multi-head attention mechanism that similar to the attention mechanism in the transformer to encode the image features. Different from the original transformer encoder, in AoANet, the feed-forward layer is replaced by the proposed Attention on Attention module (AoA). In the decoder, the AoA module is incorporated with the LSTM to predict word sequences. The AoA module is designed to determine the relevance between the attention results and the query. 


\textbf{$\mathcal{M}^2$ Transformer} \cite{cornia2020meshed} is a transformed-based method. Compared to the vanilla transformer architecture, in the $\mathcal{M}^2$ Transformer, the encoding and decoding layers are connected in a mesh-like structure to exploit both low-level and high-level contributions. Specifically, the cross-attention in the $\mathcal{M}^2$ Transformer, called meshed cross-attention, attends to all encoding layers, instead of attending only the last encoding layer. So that, multi-level contributions can be attended by this meshed cross-attention operator.




\subsection{Training and inferring methods}
All the models originally designed for image captioning adopted in this paper were trained with a phoneme-level cross-entropy loss. To investigate whether the reinforcement learning and beam search strategies, both of which show good performance in the image captioning task, work on the image-to-phoneme task or not, these strategies were also implemented in the image-to-phoneme experiments. Specifically, reinforcement learning was adopted to further fine-tune the trained models. During the inferring process, the performance of the beam search was also analyzed. Details of reinforcement learning and beam search method will be described in this section. 

\subsubsection{Reinforcement learning}
In the image captioning task, models are usually trained using the cross-entropy loss, while they are evaluated using discrete and non-differentiable NLP  metrics such as BLEU, ROUGE, METEOR, or CIDEr. Therefore, a discrepancy could exist between the training objective function and evaluation metrics. Reinforcement learning that directly optimizes metrics shows good performance to mitigate this discrepancy in the image captioning. Here, reinforcement learning was also investigated in the image-to-phoneme task. To do: method.

\subsubsection{Beam search}
During the inferring process, the auto-regressive model normally greedily selects the most probable output of the next step. Rather than only selecting one output, the beam search method will maintain a list of the N most probable sub-sequences generated so far, generate posterior probabilities for the next word of each of these sub-sequences, and then again prune down to the N-best sub-sequences. The beam search method provides a boost in the performance of image captioning. Here, we will test whether it works in the image-to-phoneme task or not.
\subsection{Evaluation metrics}
To do



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[htp]
\centering
\setlength{\tabcolsep}{1.2mm}
\caption{Image-to-phoneme results (By Justin). bs means that the beam search is adopted during the test stage. rf\_c meana the model is fine-tuned using reinforcement learning with CIDEr as reward. rf\_b means the model is fine-tuned using reinforcement learning with BLEU4 as reward.}
\label{tab:image2phone}
\begin{tabular}{l|c|cccccccccccc}
\hline
Model                                     & \multicolumn{1}{c|}{Extra} & \multicolumn{1}{c}{BLEU1} & \multicolumn{1}{c}{BLEU2} & \multicolumn{1}{c}{BLEU3} & \multicolumn{1}{c}{BLEU4} & \multicolumn{1}{c}{BLEU5} & \multicolumn{1}{c}{BLEU6} & \multicolumn{1}{c}{BLEU7} & \multicolumn{1}{c}{BLEU8} & \multicolumn{1}{c}{METEROR} & \multicolumn{1}{c}{ROUGE-L} & \multicolumn{1}{c}{CIDEr} & \multicolumn{1}{c}{PER}  \\ \hline
I2P (Justin)                                      & ---                               & \multicolumn{1}{c}{82.6}  & \multicolumn{1}{c}{62.3}  & \multicolumn{1}{c}{46.4}  & \multicolumn{1}{c}{36.1}  & \multicolumn{1}{c}{24.6}  & \multicolumn{1}{c}{18.2}  & \multicolumn{1}{c}{13.7}  & \multicolumn{1}{c}{9.3}   & \multicolumn{1}{c}{29.4}    & \multicolumn{1}{c}{49.3}    & \multicolumn{1}{c}{42.4}  & \multicolumn{1}{c}{71.4} \\ \hline
\multirow{6}{*}{SAT \cite{xu2015show}}  & ---      &   83.2   &   60.9   &   45.4   &   35.5   &   28.4   &   23.3   &   19.3   &   16.1   &   28.2   &   47.2   &   51.6   &   74.2   \\
                     & bs      &    83.8   &   64.2   &   49.9   &   40.7   &   34.0   &   28.9   &   24.7   &   21.1   &   27.1   &   48.1   &   50.1   &   70.5    \\
                      & rf\_c  &  85.7   &   64.8   &   49.2   &   39.1   &   31.8   &   26.3   &   21.9   &   18.4   &   27.2   &   47.9   &   57.8   &   70.1   \\
                      & rf\_b    &  76.1   &   62.4   &   51.4   &   43.7   &   37.5   &   32.4   &   28.1   &   24.4   &   24.2   &   47.7   &   39.0   &   68.1   \\
                     & rf\_c + bs   &   85.5   &   64.6   &   49.0   &   39.0   &   31.7   &   26.1   &   21.7   &   18.2   &   27.2   &   47.9   &   57.9   &   70.2   \\
                     & rf\_b + bs     &  75.8   &   62.1   &   51.1   &   43.4   &   37.2   &   32.1   &   27.8   &   24.1   &   24.2   &   47.5   &   38.2   &   68.3   \\ \hline
\multirow{6}{*}{Att2in \cite{rennie2017self}}                    & ---     &   83.4   &   64.5   &   51.3   &   42.2   &   35.3   &   29.8   &   25.3   &   21.6   &   30.8   &   51.5   &   64.1   &   72.7   \\
                                          & bs   &  85.1   &   68.0   &   55.4   &   46.6   &   39.9   &   34.5   &   29.9   &   25.9   &   29.3   &   51.7   &   64.2   &   68.2   \\
                                          & rf\_c    &   85.8   &   68.1   &   55.0   &   46.0   &   39.1   &   33.6   &   29.0   &   25.1   &   28.6   &   51.7   &   65.2   &   67.5     \\
                                          & rf\_b     &  81.2   &   66.6   &   54.9   &   46.6   &   40.0   &   34.7   &   30.3   &   26.4   &   25.8   &   50.0   &   46.5   &   66.6  \\
                                          & rf\_c + bs   &   84.6   &   68.4   &   56.3   &   47.9   &   41.5   &   36.2   &   31.7   &   27.7   &   27.8   &   51.5   &   60.9   &   66.2   \\
                                          & rf\_b + bs   &  77.3   &   64.2   &   53.6   &   46.2   &   40.2   &   35.3   &   31.2   &   27.5   &   24.8   &   49.2   &   42.9   &   66.7   \\ \hline
\multirow{6}{*}{Updown \cite{anderson2018bottom}}                   & ---   &  85.6   &   66.4   &   53.1   &   44.0   &   37.0   &   31.5   &   27.0   &   23.2   &   30.0   &   51.0   &   64.4   &   71.3   \\
                                          & bs    &  83.6   &   66.9   &   55.1   &   46.9   &   40.4   &   35.0   &   30.4   &   26.5   &   27.7   &   50.3   &   59.5   &   67.6   \\
                                          & rf\_c     &   86.3   &   63.0   &   46.4   &   35.8   &   28.0   &   22.2   &   17.9   &   14.6   &   27.0   &   46.3   &   57.8   &   73.4    \\
                                          & rf\_b  &  77.3   &   64.7   &   54.0   &   46.2   &   40.0   &   34.8   &   30.4   &   26.5   &   24.7   &   49.0   &   41.7   &   67.1    \\
                                          & rf\_c + bs     &   86.4   &   63.2   &   46.6   &   36.0   &   28.2   &   22.3   &   18.0   &   14.7   &   27.0   &   46.5   &   57.7   &   73.2    \\
                                          & rf\_b + bs   &  77.0   &   64.5   &   54.0   &   46.3   &   40.0   &   34.9   &   30.5   &   26.7   &   24.6   &   49.1   &   41.7   &   67.1    \\ \hline
\multirow{6}{*}{AoANet \cite{huang2019attention}}                      & ---  & 84.8   &   66.5   &   53.7   &   44.8   &   37.9   &   32.4   &   27.9   &   24.1   &   30.4   &   51.6   &   67.3   &   70.7    \\
                                          & bs    &  80.9   &   65.8   &   55.0   &   47.3   &   41.2   &   36.1   &   31.6   &   27.8   &   27.2   &   50.2   &   57.5   &   67.2    \\
                                          & rf\_c  &  86.1   &   70.4   &   58.7   &   50.3   &   43.7   &   38.1   &   33.4   &   29.3   &   29.7   &   53.5   &   72.9   &   65.3    \\
                                          & rf\_b   &  81.0   &   68.5   &   59.0   &   51.9   &   46.0   &   40.9   &   36.4   &   32.4   &   26.7   &   52.0   &   55.1   &   63.9    \\
                                          & rf\_c + bs   &   86.1   &   71.0   &   59.8   &   51.7   &   45.2   &   39.8   &   35.1   &   31.0   &   29.0   &   53.4   &   70.0   &   64.7    \\
                                          & rf\_b + bs  &  79.8   &   68.0   &   58.8   &   52.0   &   46.2   &   41.2   &   36.8   &   32.8   &   26.3   &   51.5   &   52.7   &   64.1    \\ \hline
\multirow{6}{*}{xlan \cite{pan2020x}}     & ---   & 85.5   &   67.5   &   55.0   &   46.1   &   39.0   &   33.3   &   28.5   &   24.5   &   30.2   &   51.6   &   67.6   &   70.0    \\
                                          & bs    & 84.0   &   67.5   &   56.0   &   47.8   &   41.2   &   35.8   &   31.1   &   27.1   &   28.3   &   50.7   &   64.3   &   67.6    \\
                                          & rf\_c   & 86.6   &   70.2   &   58.4   &   49.9   &   43.1   &   37.5   &   32.7   &   28.6   &   29.3   &   53.1   &   71.5   &   65.6    \\
                                          & rf\_b   & 82.2   &   68.3   &   58.1   &   50.6   &   44.4   &   39.2   &   34.6   &   30.6   &   27.7   &   52.2   &   62.2   &   64.6    \\
                                          & rf\_c + bs  &  85.1   &   69.8   &   58.8   &   50.8   &   44.4   &   38.9   &   34.2   &   30.1   &   28.5   &   52.9   &   67.4   &   64.5    \\
                                          & rf\_b + bs  &  80.5   &   67.4   &   57.6   &   50.5   &   44.6   &   39.6   &   35.2   &   31.3   &   26.9   &   51.6   &   57.5   &   64.6    \\ \hline
\multirow{6}{*}{xtransformer \cite{pan2020x}}   & ---   & 83.6   &   65.6   &   52.9   &   44.0   &   37.0   &   31.3   &   26.7   &   22.7   &   31.2   &   51.7   &   66.3   &   73.0    \\
                                          & bs    & 85.9   &   68.8   &   56.5   &   47.9   &   41.2   &   35.6   &   30.9   &   26.7   &   29.7   &   52.2   &   67.9   &   67.7    \\
                                          & rf\_c   & 86.8   &   70.6   &   58.7   &   50.2   &   43.3   &   37.6   &   32.8   &   28.6   &   29.0   &   53.3   &   69.5   &   65.5    \\
                                          & rf\_b   &  79.0   &   66.2   &   56.3   &   49.2   &   43.4   &   38.4   &   34.1    &   30.2   &   26.4   &   51.6   &   53.6   &   64.7    \\
                                          & rf\_c + bs  & 82.8   &   68.6   &   57.7   &   50.0   &   43.7   &   38.4   &   33.8   &   29.7   &   27.3   &   52.5   &   58.5   &   64.3    \\
                                          & rf\_b + bs  &  75.3   &   63.5   &   54.3   &   47.7   &   42.2   &   37.5   &   33.4   &   29.8   &   25.3   &   50.4   &   47.5   &   65.1    \\ \hline
\end{tabular}
\end{table*}













\begin{table*}[htp]
\centering
\setlength{\tabcolsep}{1.2mm}
\caption{Image-to-phoneme results (By Xinsheng). bs means that the beam search is adopted during the test stage. rf\_c meana the model is fine-tuned using reinforcement learning with CIDEr as reward. rf\_b means the model is fine-tuned using reinforcement learning with BLEU4 as reward.}
\label{tab:image2phone}
\begin{tabular}{l|c|cccccccccccc}
\hline
Model                                     & \multicolumn{1}{c|}{Extra} & \multicolumn{1}{c}{BLEU1} & \multicolumn{1}{c}{BLEU2} & \multicolumn{1}{c}{BLEU3} & \multicolumn{1}{c}{BLEU4} & \multicolumn{1}{c}{BLEU5} & \multicolumn{1}{c}{BLEU6} & \multicolumn{1}{c}{BLEU7} & \multicolumn{1}{c}{BLEU8} & \multicolumn{1}{c}{METEROR} & \multicolumn{1}{c}{ROUGE-L} & \multicolumn{1}{c}{CIDEr} & \multicolumn{1}{c}{PER}  \\ \hline
I2P (Justin)                                      & ---                               & \multicolumn{1}{c}{82.6}  & \multicolumn{1}{c}{62.3}  & \multicolumn{1}{c}{46.4}  & \multicolumn{1}{c}{36.1}  & \multicolumn{1}{c}{24.6}  & \multicolumn{1}{c}{18.2}  & \multicolumn{1}{c}{13.7}  & \multicolumn{1}{c}{9.3}   & \multicolumn{1}{c}{29.4}    & \multicolumn{1}{c}{49.3}    & \multicolumn{1}{c}{42.4}  & \multicolumn{1}{c}{71.4} \\ \hline
\multirow{6}{*}{01\_SAT}          & \_         & 78.6    & 57.0    & 42.5    & 33.2    & 26.7    & 21.8    & 18.1    & 15.1    & 29.6   & 49.8     & 53.5  \\
                                  & bs         & 84.1    & 63.9    & 49.7    & 40.6    & 34.0    & 28.9    & 24.7    & 21.1    & 29.0   & 52.2     & 64.4  \\
                                  & rf\_c      & 86.2    & 64.4    & 48.9    & 39.1    & 31.9    & 26.4    & 22.0    & 18.5    & 29.3   & 52.0     & 75.7  \\
                                  & rf\_b      & 85.9    & 69.8    & 57.5    & 48.9    & 42.0    & 36.2    & 31.4    & 27.2    & 27.1   & 54.0     & 66.8  \\
                                  & rf\_c + bs & 86.0    & 64.2    & 48.8    & 38.9    & 31.7    & 26.3    & 21.9    & 18.4    & 29.3   & 52.0     & 76.1  \\
                                  & rf\_b + bs & 85.6    & 69.5    & 57.1    & 48.5    & 41.7    & 35.9    & 31.1    & 26.9    & 27.0   & 53.8     & 65.5  \\
\multirow{6}{*}{07\_att2in}       & \_         & 78.3    & 59.9    & 47.4    & 38.8    & 32.3    & 27.2    & 23.0    & 19.5    & 31.5   & 52.9     & 57.5  \\
                                  & bs         & 84.4    & 66.9    & 54.4    & 45.8    & 39.1    & 33.7    & 29.2    & 25.2    & 31.1   & 55.3     & 74.4  \\
                                  & rf\_c      & 85.5    & 67.1    & 54.2    & 45.2    & 38.4    & 33.0    & 28.4    & 24.5    & 30.4   & 55.2     & 76.9  \\
                                  & rf\_b      & 87.6    & 71.1    & 58.5    & 49.7    & 42.7    & 37.0    & 32.3    & 28.1    & 28.4   & 55.3     & 72.5  \\
                                  & rf\_c + bs & 87.0    & 69.6    & 57.2    & 48.7    & 42.0    & 36.6    & 31.9    & 27.8    & 30.1   & 56.0     & 81.1  \\
                                  & rf\_b + bs & 86.7    & 71.3    & 59.5    & 51.2    & 44.6    & 39.2    & 34.5    & 30.4    & 27.6   & 55.3     & 71.0  \\
\multirow{6}{*}{13\_updown}       & \_         & 81.2    & 62.2    & 49.5    & 40.9    & 34.3    & 29.1    & 24.9    & 21.3    & 31.3   & 53.1     & 61.9  \\
                                  & bs         & 85.9    & 68.0    & 55.9    & 47.5    & 40.8    & 35.3    & 30.6    & 26.6    & 29.9   & 54.6     & 76.3  \\
                                  & rf\_c      & 85.8    & 62.2    & 46.0    & 35.5    & 27.9    & 22.1    & 17.9    & 14.6    & 29.0   & 50.4     & 71.2  \\
                                  & rf\_b      & 87.7    & 72.7    & 60.5    & 51.9    & 44.9    & 39.1    & 34.1    & 29.8    & 27.4   & 55.1     & 68.8  \\
                                  & rf\_c + bs & 86.2    & 62.6    & 46.3    & 35.8    & 28.1    & 22.3    & 18.0    & 14.7    & 29.0   & 50.6     & 72.0  \\
                                  & rf\_b + bs & 87.5    & 72.6    & 60.6    & 52.0    & 45.0    & 39.2    & 34.3    & 29.9    & 27.4   & 55.1     & 68.7  \\
\multirow{6}{*}{19\_aoa}          & \_         & 80.8    & 62.7    & 50.3    & 41.8    & 35.3    & 30.1    & 25.8    & 22.2    & 31.6   & 53.7     & 67.3  \\
                                  & bs         & 84.6    & 68.1    & 56.8    & 48.7    & 42.3    & 37.0    & 32.3    & 28.3    & 29.4   & 54.7     & 76.0  \\
                                  & rf\_c      & 86.6    & 70.0    & 58.2    & 49.7    & 43.0    & 37.5    & 32.7    & 28.6    & 31.6   & 56.9     & 86.5  \\
                                  & rf\_b      & 87.4    & 73.2    & 62.8    & 55.1    & 48.7    & 43.2    & 38.4    & 34.0    & 29.5   & 57.7     & 84.2  \\
                                  & rf\_c + bs & 87.8    & 71.8    & 60.2    & 51.9    & 45.3    & 39.7    & 34.9    & 30.7    & 31.2   & 57.5     & 88.9  \\
                                  & rf\_b + bs & 87.4    & 73.7    & 63.5    & 55.9    & 49.6    & 44.1    & 39.3    & 35.0    & 29.2   & 57.4     & 82.7  \\
\multirow{6}{*}{25\_xlan}         & \_         & 81.8    & 63.8    & 51.7    & 43.2    & 36.5    & 31.1    & 26.6    & 22.8    & 31.7   & 54.4     & 70.1  \\
                                  & bs         & 85.6    & 68.1    & 56.3    & 48.0    & 41.5    & 36.0    & 31.3    & 27.3    & 30.4   & 54.9     & 77.5  \\
                                  & rf\_c      & 87.0    & 69.7    & 57.7    & 49.2    & 42.5    & 36.9    & 32.2    & 28.1    & 31.3   & 56.9     & 85.1  \\
                                  & rf\_b      & 87.6    & 72.1    & 61.1    & 53.1    & 46.6    & 41.0    & 36.2    & 31.9    & 30.4   & 57.2     & 86.6  \\
                                  & rf\_c + bs & 87.6    & 71.1    & 59.6    & 51.4    & 44.8    & 39.3    & 34.5    & 30.3    & 30.9   & 57.2     & 88.1  \\
                                  & rf\_b + bs & 87.3    & 72.4    & 61.7    & 54.0    & 47.7    & 42.3    & 37.6    & 33.4    & 29.7   & 57.1     & 85.0  \\
\multirow{6}{*}{31\_xtransformer} & \_         & 78.7    & 60.9    & 48.8    & 40.2    & 33.6    & 28.3    & 24.0    & 20.3    & 32.1   & 53.3     & 60.2  \\
                                  & bs         & 84.0    & 66.3    & 54.2    & 45.8    & 39.2    & 33.8    & 29.2    & 25.2    & 31.5   & 55.6     & 75.2  \\
                                  & rf\_c      & 87.1    & 70.0    & 57.8    & 49.1    & 42.3    & 36.6    & 31.8    & 27.5    & 31.0   & 56.8     & 81.3  \\
                                  & rf\_b      & 87.3    & 72.4    & 61.3    & 53.4    & 47.0    & 41.5    & 36.7    & 32.4    & 29.2   & 57.0     & 79.6  \\
                                  & rf\_c + bs & 88.3    & 72.2    & 60.5    & 52.2    & 45.6    & 39.9    & 35.1    & 30.8    & 29.9   & 57.3     & 82.9  \\
                                  & rf\_b + bs & 85.8    & 71.7    & 61.1    & 53.6    & 47.3    & 42.0    & 37.3    & 33.1    & 28.3   & 56.5     & 76.7    \\ \hline
\end{tabular}
\end{table*}













\begin{table*}[htp]
\centering
\setlength{\tabcolsep}{1.2mm}
\caption{Image-to-phoneme results. bs means that the beam search is adopted during the test stage. rf\_c meana the model is fine-tuned using reinforcement learning with CIDEr as reward. rf\_b means the model is fine-tuned using reinforcement learning with BLEU4 as reward.}
\label{tab:image2phone}
\begin{tabular}{l|c|cccccccccccc}
\hline
Model                                     & \multicolumn{1}{c|}{Extra} & \multicolumn{1}{c}{BLEU1 by Justin} & \multicolumn{1}{c}{BLEU1 by Xinsheng} & \multicolumn{1}{c}{PER by Justin} \\ \hline
I2P (Justin)                                      & ---  & \multicolumn{1}{c}{82.6}  & \multicolumn{1}{c}{62.3}  & \multicolumn{1}{c}{71.4} \\ \hline
\multirow{6}{*}{SAT \cite{xu2015show}}  & ---      &   83.2   &  78.6 & 74.2 \\
                     & bs      &    83.8   &   84.1  & 70.5 \\
                      & rf\_c  &  85.7   &   86.2  & 70.1 \\
                      & rf\_b    &  76.1   &   85.9  & 68.1 \\
                     & rf\_c + bs   &   85.5   &   86.0 & 70.2 \\
                     & rf\_b + bs     &  75.8   &   85.6  & 68.3 \\ \hline
\multirow{6}{*}{Att2in \cite{rennie2017self}}         & ---     &   83.4   &   78.3  & 72.7 \\
                                          & bs   &  85.1   &   84.4 & 68.2\\
                                          & rf\_c    &   85.8   &   85.5 & 67.5\\
                                          & rf\_b     &  81.2   &   87.6 & 66.6 \\
                                          & rf\_c + bs   &   84.6   &   87.0 & 66.2\\
                                          & rf\_b + bs   &  77.3   &   86.7  & 66.7 \\ \hline
\multirow{6}{*}{Updown \cite{anderson2018bottom}}         & ---   &  85.6   &   81.2 & 71.3 \\
                                          & bs    &  83.6   &  85.9 & 67.6 \\
                                          & rf\_c     &   86.3   &   85.8 & 73.4 \\
                                          & rf\_b  &  77.3   &   87.7 & 67.1\\
                                          & rf\_c + bs     &   86.4   &   86.2 &73.2 \\
                                          & rf\_b + bs   &  77.0   &   87.5 &  67.1 \\ \hline
\multirow{6}{*}{AoANet \cite{huang2019attention}}           & ---  & 84.8   &   80.8  & 70.7\\
                                          & bs    &  80.9   &   84.6 & 67.2 \\
                                          & rf\_c  &  86.1   &   86.6 & 65.3 \\
                                          & rf\_b   &  81.0   &   87.4  & 63.9\\
                                          & rf\_c + bs   &   86.1   &   87.8 & 64.7 \\
                                          & rf\_b + bs  &  79.8   &  87.4  & 64.1 \\ \hline
\multirow{6}{*}{xlan \cite{pan2020x}}     & ---   & 85.5   &   81.8 & 70.0\\
                                          & bs    & 84.0   &   85.6 & 67.6\\
                                          & rf\_c   & 86.6   &   87.0 & 65.6\\
                                          & rf\_b   & 82.2   &   87.6 & 64.6 \\
                                          & rf\_c + bs  &  85.1   &   87.6 & 64.5\\
                                          & rf\_b + bs  &  80.5   &   87.3 &  64.6\\ \hline
\multirow{6}{*}{xtransformer \cite{pan2020x}}   & ---   & 83.6   &   78.7 & 73.0\\
                                          & bs    & 85.9   &   84.0 &67.7 \\
                                          & rf\_c   & 86.8   &   87.1 &65.5 \\
                                          & rf\_b   &  79.0   &   87.2  & 64.7 \\
                                          & rf\_c + bs  & 82.8   &   88.3  & 64.3\\
                                          & rf\_b + bs  &  75.3   &   85.7 & 65.1\\ \hline
\end{tabular}
\end{table*}





\begin{table*}[]
\centering
\caption{Relation with human ratings}
\setlength{\tabcolsep}{1mm}
\label{tab:rating_relation}
\begin{tabular}{l|lll|lll|lll|lll|lll}
\hline
        & \multicolumn{3}{c|}{Justin} & \multicolumn{3}{c|}{SOTA1} & \multicolumn{3}{c|}{SOTA2} & \multicolumn{3}{c|}{SOTA3} & \multicolumn{3}{c}{SOTA4} \\ \hline
        & $r$      & $r_{action}$      & $r_{object}$      & $r$     & $r_{action}$      & $r_{object}$      & $r$      & $r_{action}$     & $r_{object}$     & $r$      & $r_{action}$     & $r_{object}$      & $r$      & $r_{action}$      & $r_{object}$      \\ \hline
MTurk   &  ----       &  0.569       &  0.627      &        &         &        &        &         &        &        &         &        &        &         &        \\
BLEU1   &  0.155      &  0.214       &  0.195       &        &         &        &        &         &        &        &         &        &        &         &        \\
BLEU2   &  0.355      & 0.388        &  0.411       &        &         &        &        &         &        &        &         &        &        &         &        \\
BLEU3   &  0.425      & 0.446        &  0.486       &        &         &        &        &         &        &        &         &        &        &         &        \\
BLEU4   &  0.435      & 0.449        &  0.494       &        &         &        &        &         &        &        &         &        &        &         &        \\
BLEU5   &  0.429      & 0.435        &  0.484       &        &         &        &        &         &        &        &         &        &        &         &        \\
BLEU6   &  0.410      & 0.406        &  0.451       &        &         &        &        &         &        &        &         &        &        &         &        \\
BLEU7   &  0.378      & 0.373        &  0.423       &        &         &        &        &         &        &        &         &        &        &         &        \\
BLEU8   &  0.340      & 0.319        &  0.376       &        &         &        &        &         &        &        &         &        &        &         &        \\
METEROR &  0.258      & 0.265        &  0.322       &        &         &        &        &         &        &        &         &        &        &         &        \\
ROUGE-L &  0.425      & 0.416        &  0.485       &        &         &        &        &         &        &        &         &        &        &         &        \\
CIDEr   &  0.272      & 0.305        &  0.315       &        &         &        &        &         &        &        &         &        &        &         &        \\
PER     &  -0.361      &    -0.363     &    -0.381     &        &         &        &        &         &        &        &         &        &        &         &        \\ \hline
\end{tabular}
\end{table*}





\section{End-to-end image-to-speech}
\label{sc:End2end model}
To do.

\section{Results}
\label{sc:results}
\subsection{Dataset}
To do
\subsection{Image-to-phoneme}
To do


\subsection{End-to-end image-to-speech}
To do





\section{Discussion}
\label{sc:discussion}
To do

\section{Conclusion}
\label{sc:conclusion}
To do

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.











% use section* for acknowledgment
% \section*{Acknowledgment}


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
% \ifCLASSOPTIONcaptionsoff
 \newpage
% \fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
\bibliography{Reference.bib}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

\end{document}


