
\section{Introduction}

% dubbing 应用价值
With the development of speech processing, automatic dubbing which converts the speeches in films, televisions or games to their corresponding versions in a different language rendition,
is now a potential research area in text-to-speech (TTS) synthesis and could be utilized in many real-world scenarios such as film and game localization.

% dubbing 中的关键问题是style transfer
Differing from conventional TTS synthesis \citep{wang2017tacotron, shen2018natural, li2019neural, ren2020fastspeech, kim2021conditional} 
which %only takes texts as inputs,
directly synthesize speech from the given text,
automatic dubbing need to further consider the speaking styles in speech
and transfer them to the dubbed speech during synthesis.
%in which the speaking style plays the most important role.
In film or game production, various speaking styles are used by the actors to
present the personalities, emotions and intentions of the different characters,
which include the speaking styles at the global (i.e. utterance level) and local scale (i.e. word level).
Global speaking style \citep{wang2018style} is used to control the global emotion and intention for each utterance,
while local speaking style \citep{li_inferring_2022} is used to control the emphasis and intonation for each word to highlight the details in each utterance.
Both kinds of speaking style are 
crucial for speech understanding 
and should be transferred in automatic dubbing
to give the audiences the impression that the characters are speaking in their native tongue.
%which could be also represented as the
%global speaking style \citep{}
%and
%local speaking style \citep{}
%Such on speaking styles %should be transferred to dubbed speech
%is 
%which is also the ultimate goal of automatic dubbing.

% dubbing 现状
State-of-the-art automatic dubbing systems \citep{9747158} have successfully transferred the duration and speaking rate to dubbed speech,
which are two important aspects in local speaking style.
These researches employ algorithms based on
attention mechanism \citep{oktem2019prosodic} or hidden Markov model (HMM) \citep{federico2020evaluating, virkar2022prosodic}
to match the prosodic phrases in the two languages.
Then several prosodic phrase alignment methods \citep{oktem2019prosodic} are proposed to adjust the duration \citep{9747158}
and speaking rate for each prosodic phrase \citep{virkar2021improvements}.
to match the duration of the corresponding prosodic phrase in the speech in the original language.
%of each prosodic phrase 
%which could be further improved
%with the ability to also adjust the 

%are first separated from the source speech by pauses.
%Then different algorithms are employed for 
%to discover the corresponding prosodic phrases in the dubbed speech,
%which are further used to adjust the duration of each phoneme during synthesis.

% 现有问题1
However, these existing automatic dubbing systems are still limited and hard to transfer the other aspects in speaking style.
%First of all, 
These approaches are only applicable to very few simple dubbing scenarios 
where the numbers of prosodic phrases are same the source and dubbed languages.
%these prosodic phrase-based approaches can only be used for 
%due to requirement that 
%which are only suitable for some simple dubbing occasions.
For those complicated scenarios that the number of prosodic phrases are often different 
such as dubbing between Chinese and English, 
these approaches may synthesize improper pauses in the dubbed speeches. %and are not applicable.
% 现有问题2
Furthermore, these approaches 
can only adjust the duration and speaking rate of each prosodic phrase according to the prosodic phrases in the original language at the same index no matter if they have same semantic meanings.
Such design
could produce improper duration and speaking rate when the word orders are significantly different in the original and dubbed languages.
%in the dubbed speeches.
%also require that
%the prosodic phrases at the same index in the source and dubbed languages should 
%since they 
%For those scenarios that 
%these approaches 

% 现有问题3
Moreover, modeling on the speaking style transfer is extremely insufficient in existing automatic dubbing systems.
%have between the source and dubbed languages.
At the local scale, 
the modeling on the other aspects in local speaking styles rather than duration and speaking rate,
such as pitch, energy and emotion 
which are also crucial to present the character
is largely missing.
%only the duration and speaking rate are modeled in these dubbing algorithms.
% 现有问题4
In addition, 
the modeling on the speaking style transfer at the global scale is also rarely considered. %in existing approaches also lack 
Such insufficient modeling on the transfer of both local and global speaking styles will
synthesis speeches without those particular emotions and intentions 
and will seriously decrease the expressiveness of the dubbed speech.
%plain, boring dubbed speeches.
%cause the losing of these and information and the 

In this paper,
to address these issues and increase the expressiveness in automatic dubbing,
we propose a framework for multi-scale cross-lingual speaking style transfer, 
which jointly optimize the modeling of bidirectional speaking style transfer between two languages with multi-task learning.
%both the source to dubbed language direction and vise-versa.
In the proposed framework,
the global and local speaking styles of the speeches in different languages are extracted and utilized to predict the corresponding global and local speaking styles in the other language by an encoder-decoder framework for each direction and a shared bidirectional attention mechanism \cite{li2022neufa} for both directions.
The predicted global and local speaking styles for the other language are then synthesized to speech by
a corresponding multi-scale speaking style enhanced FastSpeech 2 (hereafter, MST-FastSpeech 2) \citep{ren2020fastspeech, li_inferring_2022}
for each language.

Experiments results %on a corpus extracted from a game
demonstrated the effectiveness of our proposed model.
The proposed approach outperform the baseline approach with only duration transfer in both
objective and subjective evaluations,
with MOS increased by $0.2$ and $0.013$,
and the preference rate exceeded by $39.64\%$ and $3.58\%$
on the two dubbing directions.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\linewidth]{figure/Figure2.pdf}
	\caption{Cross-lingual speaking style transfer between two languages at multiple scales}
	\label{fig:emoji}
\end{figure}

%on a corpus extracted from a game with professional English and Chinese dubs.

% 现有问题5
%Besides the original dubbing direction,
%the reversed dubbing direction which %converts the dubbed speech back to the source speech,
%creates dubbing of the source language for the target language,
%could be also considered to further optimize the speaking style transfer for both directions.
%Some patterns of cross-lingual speaking style transfer are shared between the two directions,
%which could be jointly modeled by multi-task learning and bidirectional models such as bidirectional attention mechanism \citep{li2022neufa, vaswani2017attention}.

%multi-task learning \citep{} could be used to further improve the modeling on speaking style transfer
%when jointly considering dubbing process in the both directions -- %such as Chinese dubbing for English and English dubbing for Chinese.
%i.e. the dubbing the target language for the source language and vise-versa.
%There are some shared pattern between the speaking style transfer for these two reversed directions and.
%are found to have during observation.


%i bidirectional attention mechanism \citep{neufa} and multi-task learning
%are employed to 

\iffalse
%The rest sections of the paper are organized as the follows.
The rest of the paper is organized as follows.
%We introduce the researches that related to our work and their context modeling method 
We introduce related work in Section \ref{related}. 
We then conduct data observation
%on conversations 
in Section \ref{observation}.
% and the pattens learnt from the observation.
%and
%formulate the problem
%%processes 
%of context modeling and conversational TTS in Section \ref{problem}. 
In Section \ref{method}, we introduce the details of proposed approach.
Experimental results and ablation studies are presented in Section \ref{experiments}.
Finally, %conclusion is made in 
Section \ref{conclusion}
concludes the paper.
\fi